---
title: "Building a RAG application with locally executed LLMs on LangChain"
author: "Alex Gonzalez"
date: "2024-04-02"
description: Creating a question answering chatbot using Large Language Models on your own computer without the need for external API calls is not as hard as it sounds!
categories: [LLM, langchain]
image: "image.jpg"
draft: true
---

One of the latest trending methods in modern NLP is Retrieval Augmented Generation (RAG), which is a recent approach to improve the information retrieval and generation capabilities of large language models. In a RAG system, the models are used to extract relevant information from large datasets and generate coherent responses and insights from them. This is a invaluable technique for several common use cases such as question answering or conversational agents. However, cutting-edge LLMs take vast amount of resources for training and inference and are not usually free to access.

In this article, we will walk through the process of running an LLM locally and using it to build our RAG application. This application will consist of a simple chatbot that can retrieve context from a knowledge base containing documents related to the award-winning videogame Elden Ring.


## A Primer on RAG Systems

RAG systems combine the strengths of infomation retrieval and natural language to effectively understand and generate human-like responses. Unexpectedly, the two key components of such systems are **retrieval** and **generation**.

### Retrieval

In the retrieval step, the system searches its knowledge base for hits matching a user's query. The knowledge base should be stored in a vector database with embeddings for efficient search with techniques such as semantic similarity measures.
The goal of this step is to extract the most relevant pieces of information given the original prompt, and those that could help in generating a coherent response. Once the context information is collected, the generation step can use it for further processing.

### Generation

In the generation step, the system feeds the obtained context to an LLM, such as GPT or BERT, that can generate coherent output based on the provided information. Coupling the natural language understanding of the model with the context results in responses that are gramatically correct, contextually relevant and coherent.

## Application Design

Before writing the first line of code, let us take some time to design our application and define its scope.

## Development Environment Setup

::: {.callout-warning}
LLM inference needs a lot of computing power, so make sure your machine can handle it before continuing. For reference, I ran this on a Windows laptop with an 11th Gen i7 chip and 32GB RAM.
If you have similar specs (or a GPU) in your computer, you are probably good to go.
:::

You will need the following stuff in order to run the code in this guide:

- [LangChain](https://github.com/langchain-ai/langchain): a Python library that provides a framework for building LLM-powered applications
- a large language model file: [TheBloke](https://huggingface.co/TheBloke)'s HuggingFace page has already many models which are already quantized, lifting some of the process from your machine. Use the 7B or 13B parameter models depending on your system.

Then, let's create a virtual environment with the necessary requirements:

```bash
python -m venv .venv
source .venv/bin/activate
pip install langchain docarray
```

## Prepare the Knowledge Base

In order to feed your LLM of choice with the required context, you will need a knowledge base that can be parsed and incorporated into a vector database.
For this guide, I will use the data available from the [Elden Ring Explorer](https://eldenringexplorer.github.io/EldenRingTextExplorer/elden_ring_text.json) project.

### Quick and dirty data engineering

The dataset contains many fields that are not relevant to the task, so let us clean the data and prepare it for our chatbot.

## Retrieval Setup


## LLM Setup
### Choosing an LLM

## Build the RAG system

## Fine-Tuning, Optimization and Advanced Features

## Common Challenges and Pitfalls


## Conclusion

That's it! Your very own RAG application now lives and runs on your computer (if it doesn't spontaneously combust in the process) and you can test it with your own data as well.
The advent of somewhat easily accesible LLMs opens up a myriad possibilities for new projects and ideas. It is a constantly evolving landscape and you can get creative with stuff like chatbots, research assistants, or content generators. The skills from this guide are fairly basic but building on them can take your NLP game to the next level.

Now go forth and deploy something cool with RAG tech!