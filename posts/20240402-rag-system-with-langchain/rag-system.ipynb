{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---\n",
    "title: \"Understanding RAG systems with a practical application using locally executed LLMs\"\n",
    "subtitle: \"Explore the lore of Elden Ring through natural language\"\n",
    "author: \"Alex Gonzalez\"\n",
    "date: \"2024-04-02\"\n",
    "categories: [AI, Code]\n",
    "image: \"image.jpg\"\n",
    "reference-location: margin\n",
    "draft: false\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "One of the latest trending methods in modern NLP is **Retrieval Augmented Generation** (RAG), which is a recent approach to improve the information retrieval and generation capabilities of large language models. In a RAG system, the models are used to extract relevant information from large datasets and generate coherent responses and insights from them. This is a invaluable technique for several common use cases such as question answering or conversational agents. However, cutting-edge LLMs take vast amount of resources for training and inference and are not usually free to access.\n",
    "\n",
    "## Motivation\n",
    "\n",
    "One of the most typical applications of RAG is question answering over corporate documents, such as company guidelines, product reviews, or documentation. While these applications are common, I prefer to explore more unconventional data sources that may lead to more engaging projects.\n",
    "\n",
    "In this article, we will walk through the process of running an LLM locally and using it to build our RAG application. This application will consist of a simple chatbot that can retrieve context from a knowledge base containing documents related to the award-winning videogame *Elden Ring*. \n",
    "\n",
    "### Background: Elden Ring\n",
    "\n",
    "*Elden Ring* is renowned for its intricate and immersive story, with narrative elements such as environmental storytelling, lore and mythos found in item, skill and equipment descriptions; and NPC interactions. Much of this story is left open to the player's interpretation of cryptic dialogue and obscure descriptions, as well as drawing connections between disparate elements to create a cohesive narrative of the world.\n",
    "\n",
    "With all that preamble out of the way, let's create a chatbot that serves as an expert in the lore of *Elden Ring* and can assist the user in exploring the different connections in the game's rich storytelling.\n",
    "\n",
    "## A Primer on RAG Systems\n",
    "\n",
    "::: {.callout-note}\n",
    "If you want to skip the theory and preprocessing, go straight [here](#build-the-rag-system).\n",
    ":::\n",
    "\n",
    "RAG systems combine the strengths of infomation retrieval and natural language to effectively understand and generate human-like responses. Not unexpectedly, the two key components of such systems are **retrieval** and **generation**.\n",
    "\n",
    "### Retrieval\n",
    "\n",
    "In the retrieval step, the system searches its knowledge base for hits matching a user's query. The knowledge base should be stored in a vector database with embeddings for efficient search with techniques such as semantic similarity measures.\n",
    "The goal of this step is to extract the most relevant pieces of information given the original prompt, and those that could help in generating a coherent response. Once the context information is collected, the generation step can use it for further processing.\n",
    "\n",
    "### Generation\n",
    "\n",
    "In the generation step, the system feeds the obtained context to an LLM, such as GPT or BERT, that can generate coherent output based on the provided information. Coupling the natural language understanding of the model with the context results in responses that are gramatically correct, contextually relevant and coherent.\n",
    "\n",
    "## Application Design\n",
    "\n",
    "Before writing the first line of code, let us take some time to design our application and define its scope.\n",
    "\n",
    "## Development Environment Setup\n",
    "\n",
    "::: {.callout-warning}\n",
    "LLM inference needs a lot of computing power, so make sure your machine can handle it before continuing. For reference, I ran this on a Windows laptop with an 11th Gen i7 chip and 32GB RAM.\n",
    "If you have better or similar specs (or GPU) in your computer, you are probably good to go.\n",
    ":::\n",
    "\n",
    "You will need the following stuff in order to run the code in this guide:\n",
    "\n",
    "- [LangChain](https://github.com/langchain-ai/langchain): a Python library that provides a framework for building LLM-powered applications\n",
    "- a large language model file: [TheBloke](https://huggingface.co/TheBloke)'s HuggingFace page already has many models that are already [quantized](https://huggingface.co/docs/optimum/en/concept_guides/quantization), lifting some of the process from your machine. Use the 7B or 13B parameter models depending on your system. More on this [further below](#choosing-an-llm).\n",
    "\n",
    "Then, let's create a virtual environment with the necessary requirements:\n",
    "\n",
    "```bash\n",
    "python -m venv .venv\n",
    "source .venv/bin/activate\n",
    "pip install langchain docarray pandas\n",
    "```\n",
    "\n",
    "## Prepare the Knowledge Base\n",
    "\n",
    "In order to feed your LLM of choice with the required context, you will need a knowledge base that can be parsed and incorporated into a vector database.\n",
    "For this guide, I will use the data available from the excellent [Elden Ring Explorer](https://eldenringexplorer.github.io/EldenRingTextExplorer/) project, which in turn comes from the [Carian Archive](https://github.com/AsteriskAmpersand/Carian-Archive).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import requests\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_url = \"https://eldenringexplorer.github.io/EldenRingTextExplorer/elden_ring_text.json\"\n",
    "response = requests.get(data_url)\n",
    "\n",
    "data = response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"name_en\": \"Petition for Help\",\n",
      "  \"name_jp\": \"\\u6551\\u63f4\\u306e\\u8acb\\u9858\\u66f8\",\n",
      "  \"info_en\": \"Summons Stalker to face invading Broken Finger\",\n",
      "  \"info_jp\": \"\\u6f70\\u308c\\u6307\\u306b\\u4fb5\\u5165\\u3055\\u308c\\u305f\\u6642\\u3001\\u6f70\\u308c\\u72e9\\u308a\\u3092\\u6551\\u63f4\\u53ec\\u559a\\u3059\\u308b\",\n",
      "  \"caption_en\": \"Online multiplayer item. Receipt of a plea for\\r\\nhelp to the maidens of the Finger Reader.\\r\\n\\r\\nSummons a Broken Finger Stalker from another\\r\\nworld to face an invading Broken Finger.\\r\\n\\r\\nMaidens of the Finger Reader speak in hushed\\r\\ntones about the loathsome, traitorous Broken\\r\\nFingers and the dangers of their base invasions.\",\n",
      "  \"caption_jp\": \"\\u30aa\\u30f3\\u30e9\\u30a4\\u30f3\\u30d7\\u30ec\\u30a4\\u5c02\\u7528\\u30a2\\u30a4\\u30c6\\u30e0\\r\\n\\u6307\\u8aad\\u307f\\u306e\\u5deb\\u5973\\u305f\\u3061\\u306b\\u8acb\\u9858\\u3057\\u305f\\u8a3c\\r\\n\\r\\n\\u6f70\\u308c\\u6307\\u306b\\u4fb5\\u5165\\u3055\\u308c\\u305f\\u6642\\r\\n\\u4ed6\\u4e16\\u754c\\u304b\\u3089\\u6f70\\u308c\\u72e9\\u308a\\u3092\\u6551\\u63f4\\u53ec\\u559a\\u3059\\u308b\\r\\n\\r\\n\\u6307\\u8aad\\u307f\\u306e\\u5deb\\u5973\\u306f\\u3001\\u58f0\\u3092\\u6f5c\\u3081\\u8a9e\\u308b\\u3060\\u308d\\u3046\\r\\n\\u6f70\\u308c\\u6307\\u306e\\u3001\\u553e\\u68c4\\u3059\\u3079\\u304d\\u88cf\\u5207\\u308a\\u3068\\r\\n\\u5351\\u52a3\\u306a\\u4fb5\\u5165\\u306e\\u5371\\u3046\\u3055\\u3092\\r\\n\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Peek at the data format\n",
    "for key1 in data.keys():\n",
    "    for key2 in data[key1]:\n",
    "        print(\n",
    "            json.dumps(data[key1][key2], indent=2)\n",
    "        )\n",
    "        break\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Quick and dirty data engineering\n",
    "\n",
    "The keys in the JSON object are categories, and every category contains nested dictionaries that describe a document. This dataset contains many fields that are not relevant to the task, so let us clean the data and prepare it so that we can give the chatbot a reliable knowledge base."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = [c for c in data.keys()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>name_en</th>\n",
       "      <th>name_jp</th>\n",
       "      <th>info_en</th>\n",
       "      <th>info_jp</th>\n",
       "      <th>caption_en</th>\n",
       "      <th>caption_jp</th>\n",
       "      <th>category</th>\n",
       "      <th>effect_en</th>\n",
       "      <th>effect_jp</th>\n",
       "      <th>dialog_en</th>\n",
       "      <th>dialog_jp</th>\n",
       "      <th>type</th>\n",
       "      <th>form</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100</td>\n",
       "      <td>Petition for Help</td>\n",
       "      <td>救援の請願書</td>\n",
       "      <td>Summons Stalker to face invading Broken Finger</td>\n",
       "      <td>潰れ指に侵入された時、潰れ狩りを救援召喚する</td>\n",
       "      <td>Online multiplayer item. Receipt of a plea for...</td>\n",
       "      <td>オンラインプレイ専用アイテム\\r\\n指読みの巫女たちに請願した証\\r\\n\\r\\n潰れ指に侵入...</td>\n",
       "      <td>accessories</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>101</td>\n",
       "      <td>Broken Finger Stalker Contract</td>\n",
       "      <td>潰れ狩りの誓約書</td>\n",
       "      <td>Be summoned to worlds invaded by Broken Fingers</td>\n",
       "      <td>潰れ指に侵入された世界に救援召喚される</td>\n",
       "      <td>Online multiplayer item. Record of contract wi...</td>\n",
       "      <td>オンラインプレイ専用アイテム\\r\\n指読みの巫女たちと誓約した証\\r\\n\\r\\n他プレイヤー...</td>\n",
       "      <td>accessories</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1000</td>\n",
       "      <td>Crimson Amber Medallion</td>\n",
       "      <td>緋琥珀のメダリオン</td>\n",
       "      <td>Raises maximum HP</td>\n",
       "      <td>ＨＰの最大値を上昇させる</td>\n",
       "      <td>A medallion with crimson amber inlaid.\\r\\nBoos...</td>\n",
       "      <td>緋色の琥珀が嵌めこまれたメダリオン\\r\\nＨＰの最大値を上昇させる\\r\\n\\r\\n琥珀とは、...</td>\n",
       "      <td>accessories</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1001</td>\n",
       "      <td>Crimson Amber Medallion +1</td>\n",
       "      <td>緋琥珀のメダリオン＋１</td>\n",
       "      <td>Greatly raises maximum HP</td>\n",
       "      <td>ＨＰの最大値を大きく上昇させる</td>\n",
       "      <td>A medallion with crimson amber inlaid.\\r\\nGrea...</td>\n",
       "      <td>緋色の琥珀が嵌めこまれたメダリオン\\r\\nＨＰの最大値を大きく上昇させる\\r\\n\\r\\n琥珀...</td>\n",
       "      <td>accessories</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1002</td>\n",
       "      <td>Crimson Amber Medallion +2</td>\n",
       "      <td>緋琥珀のメダリオン＋２</td>\n",
       "      <td>Vastly raises maximum HP</td>\n",
       "      <td>ＨＰの最大値を、とても大きく上昇させる</td>\n",
       "      <td>A medallion with crimson amber inlaid.\\r\\nVast...</td>\n",
       "      <td>緋色の琥珀が嵌めこまれたメダリオン\\r\\nＨＰの最大値を、とても大きく上昇させる\\r\\n\\r...</td>\n",
       "      <td>accessories</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  index                         name_en      name_jp  \\\n",
       "0   100               Petition for Help       救援の請願書   \n",
       "1   101  Broken Finger Stalker Contract     潰れ狩りの誓約書   \n",
       "2  1000         Crimson Amber Medallion    緋琥珀のメダリオン   \n",
       "3  1001      Crimson Amber Medallion +1  緋琥珀のメダリオン＋１   \n",
       "4  1002      Crimson Amber Medallion +2  緋琥珀のメダリオン＋２   \n",
       "\n",
       "                                           info_en                 info_jp  \\\n",
       "0   Summons Stalker to face invading Broken Finger  潰れ指に侵入された時、潰れ狩りを救援召喚する   \n",
       "1  Be summoned to worlds invaded by Broken Fingers     潰れ指に侵入された世界に救援召喚される   \n",
       "2                                Raises maximum HP            ＨＰの最大値を上昇させる   \n",
       "3                        Greatly raises maximum HP         ＨＰの最大値を大きく上昇させる   \n",
       "4                         Vastly raises maximum HP     ＨＰの最大値を、とても大きく上昇させる   \n",
       "\n",
       "                                          caption_en  \\\n",
       "0  Online multiplayer item. Receipt of a plea for...   \n",
       "1  Online multiplayer item. Record of contract wi...   \n",
       "2  A medallion with crimson amber inlaid.\\r\\nBoos...   \n",
       "3  A medallion with crimson amber inlaid.\\r\\nGrea...   \n",
       "4  A medallion with crimson amber inlaid.\\r\\nVast...   \n",
       "\n",
       "                                          caption_jp     category effect_en  \\\n",
       "0  オンラインプレイ専用アイテム\\r\\n指読みの巫女たちに請願した証\\r\\n\\r\\n潰れ指に侵入...  accessories       NaN   \n",
       "1  オンラインプレイ専用アイテム\\r\\n指読みの巫女たちと誓約した証\\r\\n\\r\\n他プレイヤー...  accessories       NaN   \n",
       "2  緋色の琥珀が嵌めこまれたメダリオン\\r\\nＨＰの最大値を上昇させる\\r\\n\\r\\n琥珀とは、...  accessories       NaN   \n",
       "3  緋色の琥珀が嵌めこまれたメダリオン\\r\\nＨＰの最大値を大きく上昇させる\\r\\n\\r\\n琥珀...  accessories       NaN   \n",
       "4  緋色の琥珀が嵌めこまれたメダリオン\\r\\nＨＰの最大値を、とても大きく上昇させる\\r\\n\\r...  accessories       NaN   \n",
       "\n",
       "  effect_jp dialog_en dialog_jp type form   id  \n",
       "0       NaN       NaN       NaN  NaN  NaN  NaN  \n",
       "1       NaN       NaN       NaN  NaN  NaN  NaN  \n",
       "2       NaN       NaN       NaN  NaN  NaN  NaN  \n",
       "3       NaN       NaN       NaN  NaN  NaN  NaN  \n",
       "4       NaN       NaN       NaN  NaN  NaN  NaN  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "category_data = []\n",
    "for c in categories:\n",
    "    df = pd.DataFrame(data[c]).T\n",
    "    df[\"category\"] = c\n",
    "    category_data.append(df)\n",
    "\n",
    "df = pd.concat(category_data).reset_index()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will focus only on the English texts and keep some other metadata. Moreover, only some of the categories contain documents that might help us give the chatbot the necessary knowledge base:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "keep_columns = [\n",
    "    \"id\",\n",
    "    \"index\",\n",
    "    \"name_en\",\n",
    "    \"info_en\",\n",
    "    \"caption_en\",\n",
    "    \"dialog_en\",\n",
    "    \"type\",\n",
    "    \"form\",\n",
    "    \"category\",\n",
    "]\n",
    "\n",
    "df = df[keep_columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "::: {.callout-note}\n",
    "In an earlier version of this application, I chose to remove some categories such as those containing system messages and mechanic descriptions unrelated to the lore. However, upon testing the system, I found that some documents in those categories *did* contain relevant snippets of information.\n",
    "\n",
    "Take these nuances into account when developing your prototypes!\n",
    ":::\n",
    "\n",
    "The data cleaning process will be as follows:\n",
    "\n",
    "1. Remove rows with null values in the `name_en` column.\n",
    "2. Rename this column as the document `title`.\n",
    "3. Concatenate `info_en` and `caption_en`.\n",
    "4. Coalesce `caption_en` and `dialog_en`.\n",
    "\n",
    "After this, the resulting data will be nicely transformed into a simpler format containing only the document title, its content and category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna(subset=\"name_en\")\n",
    "df[\"title\"] = df[\"name_en\"]\n",
    "\n",
    "df[\"info_caption\"] = df[\"info_en\"].fillna(\"\") + \". \" + df[\"caption_en\"].fillna(\"\")\n",
    "df[\"description\"] = df[\"info_caption\"].combine_first(df[\"dialog_en\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the text strings in the `description` column contain extraneous characters and unwanted tokens. Let's clean those too:\n",
    "\n",
    "- Some of the descriptions contain tagged IDs, e.g. `[9000010]`\n",
    "- Some descriptions contain the token `(dummyText)`\n",
    "- There are some duplicated descriptions in the case of upgraded items (e.g \"Black Knife Tiche +4\" and \"Black Knife Tiche +5\" have the same description.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text(df: pd.DataFrame, col_name: str):\n",
    "    df.loc[:, col_name] = [re.sub(r'\\[\\d+\\]', '', x) for x in df[col_name]]\n",
    "    df.loc[:, col_name] = [re.sub(r'\\(dummyText\\)', '', x) for x in df[col_name]]\n",
    "    df = df.drop_duplicates(subset=[col_name])\n",
    "\n",
    "    return df\n",
    "\n",
    "df = process_text(df, \"description\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, select the relevant columns and save the resulting dataframe. Let's also run a few sanity checks to spot leftover nulls or duplicates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = df[[\n",
    "    \"title\",\n",
    "    \"description\",\n",
    "    \"category\",\n",
    "]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "title          0\n",
       "description    0\n",
       "category       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[\"description\"].duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice! The documents are now clean and easy to parse. To make it easier to use the data in other tasks later on, I'll save it again as a JSON/dictionary. This way, it can be smoothly integrated into different processes, especially when the data processing and chatbot logics are kept separate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = output.to_dict(orient=\"records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the RAG system\n",
    "\n",
    "Now onto the cool(er) stuff!\n",
    "\n",
    "Let's summarize how a RAG system works in three (very) simplified steps:\n",
    "\n",
    "1. The user prompts the system with a question.\n",
    "2. The question is matched against the knowledge base, which is already transformed through embeddings in the vectorstore.\n",
    "3. The returned context is fed to the LLM which can now return an informed response based on the given context.\n",
    "\n",
    "The first thing we will need to work on is how to store the documents so that the system can find them in the efficiently, and semantically. Using embeddings and vectorstores with LangChain is almost trivial since they are mostly plug-and-play."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieval Setup\n",
    "\n",
    "Let's import the libraries needed for this step, mostly related to LangChain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.docstore.document import Document\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import DocArrayInMemorySearch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a `JSONLoader()` class available in LangChain, but I found it easier to just create the individual documents since they are not many and we purposefully prepared it in a tidy format.\n",
    "For each record in the data, a document is created containing the `description` field, while the `title` and `category` are kept as metadata:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='Boosts dexterity, raises attack power with successive attacks. Part of the golden prosthesis used by Millicent.\\r\\nThe hand is locked into a fist that once raised a sword aloft.\\r\\n\\r\\nBoosts dexterity and raises attack power with successive attacks.\\r\\n\\r\\nThe despair of sweet betrayal transformed Millicent from a mere bud into a magnificent flower. And one day, she will be reborn—as a beautiful scarlet valkyrie.', metadata={'title': \"Millicent's Prosthesis\", 'category': 'accessories'})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = []\n",
    "for record in data:\n",
    "    new_document = Document(\n",
    "        page_content=record[\"description\"],\n",
    "        metadata={\"title\": record[\"title\"], \"category\": record[\"category\"]},\n",
    "    )\n",
    "    docs.append(new_document)\n",
    "\n",
    "docs[42]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to use pretrained embeddings to transform the documents when loading them into the vectorstore. I'll just use the HuggingFace embbedings but there are many other options available in LangChain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/c/Users/alejandro.gonzalez/dev/sandbox/erdbot/.venv/lib/python3.10/site-packages/pydantic/_migration.py:283: UserWarning: `pydantic.error_wrappers:ValidationError` has been moved to `pydantic:ValidationError`.\n",
      "  warnings.warn(f'`{import_path}` has been moved to `{new_location}`.')\n"
     ]
    }
   ],
   "source": [
    "#| output: false\n",
    "embeddings = HuggingFaceEmbeddings()\n",
    "\n",
    "db = DocArrayInMemorySearch.from_documents(docs, embeddings)\n",
    "retriever = db.as_retriever(search_type=\"mmr\", search_kwargs={\"k\": 5});"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the documents are vectorized, they are loaded into the vectorstore which we can use as a retriever for our chatbot chain. Some insight into the parameters chosen here:\n",
    "\n",
    "- `search_type=\"mmr\"`: stands for *Maximum Marginal Relevance*, a search measure that aims to reduce redundancy and increase diversity in search results\n",
    "- `k=5`: the number of hits to return, using 5 in this case to avoid retrieving possibly unrelated context or surpassing the input token limit\n",
    "\n",
    "Behind the scenes, the retriever will perform similarity search and return the results to the chat agent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Gauntlets used by the Black Blade Assassins. Gauntlets used by the Black Knife Assassins.\\r\\nCrafted with scale armor that makes no sound.\\r\\n\\r\\nThe assassins that carried out the deeds of the Night of the Black Knives were all women, and rumored to be Numen who had close ties with Marika herself.', metadata={'title': 'Black Knife Gauntlets', 'category': 'protector'}),\n",
       " Document(page_content='. Dagger once belonging to one of the assassins who murdered Godwyn the Golden on the Night of the Black Knives.\\r\\n\\r\\nA ritual performed on the oddly misshapen blade imbued it with the power of the stolen Rune of Death.', metadata={'title': 'Black Knife', 'category': 'weapon'}),\n",
       " Document(page_content='Simple map showing location of black knifeprint\\r\\nExamine using <?keyicon@31?>. A simple map given by Fia.\\r\\n\\r\\nA clue to the whereabouts of a black knifeprint.', metadata={'title': 'Knifeprint Clue', 'category': 'goods'}),\n",
       " Document(page_content='. Dagger with a bloodstained blade.\\r\\nAfflicts targets with blood loss.\\r\\n\\r\\nAs blood darkened the dagger through repeated slashing and stabbing, its blade only grew sharper and harder.', metadata={'title': 'Bloodstained Dagger', 'category': 'weapon'}),\n",
       " Document(page_content='Armor used by the Black Blade Assassins. Armor used by the Black Knife Assassins.\\r\\nCrafted with scale armor that makes no sound.\\r\\n\\r\\nThe assassins that carried out the deeds of the Night of the Black Knives were all women, and rumored to be Numen who had close ties with Marika herself.', metadata={'title': 'Black Knife Armor (Altered)', 'category': 'protector'})]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.similarity_search(\"Black Knife\", search_type=\"mmr\", k=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM Setup\n",
    "\n",
    "Language models are usually tuned so that they excel at specific tasks, such as text summarization or translation. In this case, we would need a **question answering** model for optimal results, but you can also use general models such as GPT.\n",
    "\n",
    "Since we are aiming to use a local LLM, there are some extra steps we need to take:\n",
    "\n",
    "### Run the LLM locally\n",
    "\n",
    "#### 1. Download the GGUF model file\n",
    "\n",
    "[GGUF files](https://huggingface.co/docs/hub/gguf#gguf) are already quantized which helps the model speed up inference. For this project, I used [WizardLM-13B](https://huggingface.co/TheBloke/WizardLM-13B-Uncensored-GGUF) but you can use smaller models such a the [7B](https://huggingface.co/TheBloke/WizardLM-7B-uncensored-GGUF) version which will run faster but perform worse.\n",
    "\n",
    "As for the quantization level, [Q4_K_M](https://huggingface.co/TheBloke/WizardLM-7B-uncensored-GGUF?show_tensors=WizardLM-7B-uncensored.Q4_K_M.gguf) is a good option from my limited experience, because the task doesn't require a very high degree precision and correctness, so lower quant levels may be acceptable. For other tasks that might require more precision such as coding, higher quants (or none at all) should be used.\n",
    "\n",
    "#### 2. Compile the model\n",
    "\n",
    "The  LlamaCpp class allows for the model to be loaded and easily interfaced with other LangChain components:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 19 key-value pairs and 291 tensors from ../../../sandbox/erdbot/models/WizardLM/wizardlm-7b-v1.0-uncensored.Q4_K_M.gguf (version GGUF V2)\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = ehartford_wizardlm-7b-v1.0-uncensored\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 2048\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  17:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  18:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V2\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 2048\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 32\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: n_embd_k_gqa     = 4096\n",
      "llm_load_print_meta: n_embd_v_gqa     = 4096\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 11008\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 2048\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 6.74 B\n",
      "llm_load_print_meta: model size       = 3.80 GiB (4.84 BPW) \n",
      "llm_load_print_meta: general.name     = ehartford_wizardlm-7b-v1.0-uncensored\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.11 MiB\n",
      "llm_load_tensors:        CPU buffer size =  3891.24 MiB\n",
      "..................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 2048\n",
      "llama_new_context_with_model: n_batch    = 8\n",
      "llama_new_context_with_model: n_ubatch   = 8\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =  1024.00 MiB\n",
      "llama_new_context_with_model: KV self size  = 1024.00 MiB, K (f16):  512.00 MiB, V (f16):  512.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.98 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =     2.56 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1060\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | \n",
      "Model metadata: {'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'general.architecture': 'llama', 'llama.context_length': '2048', 'general.name': 'ehartford_wizardlm-7b-v1.0-uncensored', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '11008', 'llama.attention.layer_norm_rms_epsilon': '0.000001', 'llama.rope.dimension_count': '128', 'llama.attention.head_count': '32', 'tokenizer.ggml.bos_token_id': '1', 'llama.block_count': '32', 'llama.attention.head_count_kv': '32', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'llama', 'general.file_type': '15'}\n",
      "Using fallback chat format: None\n"
     ]
    }
   ],
   "source": [
    "#| output: false\n",
    "\n",
    "from langchain_community.llms import LlamaCpp\n",
    "\n",
    "llm = LlamaCpp(\n",
    "    model_path=\"../../../sandbox/erdbot/models/WizardLM/wizardlm-7b-v1.0-uncensored.Q4_K_M.gguf\",\n",
    "    temperature=0.3,\n",
    "    max_tokens=4096,\n",
    "    n_ctx=2048,\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternative: Use the Hugging Face Inference API\n",
    "\n",
    "If you don't have enough computing resources, you can substitute the LLM component with the ChatHuggingFace class and use the free (and rate-limited) [Inference API](https://huggingface.co/docs/api-inference/en/index) provided by HF.\n",
    "\n",
    "You can also use any other API keys if you have access to other AI providers such as OpenAI or Azure OpenAI. Just use their the corresponding LangChain interfaces and swap the component in the chain. **This guide will only cover the local and Inference API cases.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| eval: false\n",
    "# Not executed: alternative to the local LLM\n",
    "# Note that in this case, the chain expects a ChatModel object and not a text LLM\n",
    "from langchain_community.llms import HuggingFaceEndpoint\n",
    "from langchain.chat_models import ChatHuggingFace\n",
    "\n",
    "\n",
    "llm = HuggingFaceEndpoint(\n",
    "    repo_id=\"deepset/roberta-base-squad2\",\n",
    "    huggingfacehub_api_token=HUGGINGFACEHUB_API_TOKEN,\n",
    "    model_kwargs={\"max_length\": -1}, # unlimited response length to avoid truncated answers\n",
    ")\n",
    "\n",
    "chat_model = ChatHuggingFace(llm=llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can choose from the many models available on the HF Hub, instead of being limited to only quantized models such as in the local case. For this example, I just used [roberta-base for QA](https://huggingface.co/deepset/roberta-base-squad2).\n",
    "\n",
    "## Design the Language Chain\n",
    "\n",
    "There are three main components for a chain: the prompt template, the language model, and (optionally) an output parser.\n",
    "\n",
    "LangChain's API can be somewhat obtuse at times, especially when building chains with multiple arguments, so I'll try my best to explain what's going on.\n",
    "\n",
    "### Prompt Design and Engineering\n",
    "\n",
    "Some research (i.e. [Reynolds & McDonell, 2021](https://arxiv.org/abs/2102.07350)) on LLMs shows that a carefully crafted and directed prompt can yield better results. In fact, a whole new subfield of prompt engineering is emerging, contributing to the state of the art with [methods](https://www.promptingguide.ai/techniques) such as few shot learning, chain-of-thought and self-reflexion.\n",
    "\n",
    "Furthermore, prompt design seems to be a very iterative process. As you will discover when developing your own applications, it often takes a good few tries to create a prompt that consistenly generates the desired outputs. After some time tweaking the template, I came up with the following prompt for our loremaster chatbot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "template = \"\"\"\n",
    "You are an expert historian studying the lore of an ancient civilization. \n",
    "To answer the user's question, use the following context:\n",
    "{context}\n",
    "\n",
    "Only include contextual information that not relevant to the user's question in your answer.\n",
    "If you can't infer an answer based on the provided context, explicitly say so. \n",
    "Do not invent or hallucinate your responses but try to find likely relationships and connections among the documents.\n",
    "Be concise but thorough, and use no more than 5 sentences in your response.\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some things to note:\n",
    "\n",
    "- The prompt describes the role the chat agent should take. Other applications might need different agents roles such as \"a helpful assistant\" or \"a teacher grading a math submission\". These should be tweaked based on your use case.\n",
    "- I tried to find a balance between making likely connections between semantically unrelated documents and avoid hallucinations.\n",
    "- Notice the f-string formatted variables `{context}` and `{question}`, which are a placeholder for the prompt inputs.\n",
    "\n",
    "### Chains with LCEL\n",
    "\n",
    "The next step is to build the actual language chain that will mesh together all the components of the system. In this example, I will be using the [LangChain Expression Language](https://python.langchain.com/docs/expression_language/) (LCEL) that allows the usage of the pipe (`|`) operator to chain operators and enhance readability. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableParallel\n",
    "from operator import itemgetter\n",
    "\n",
    "context = itemgetter(\"question\") | retriever\n",
    "answer = prompt | llm\n",
    "\n",
    "chain = {\n",
    "    \"context\": context,\n",
    "    \"question\": itemgetter(\"question\"),\n",
    "} | RunnableParallel({\"answer\": answer, \"context\": context})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `context = itemgetter(\"question\") | retriever`: the context is the result of sending the question to the vector database (the retriever) and getting document matches back\n",
    "- `\"question\": itemgetter(\"question\")`: this simply grabs the question (the user input) from the prompt\n",
    "- `answer = prompt | llm`: the answer is the result of passing the prompt through the LLM\n",
    "- `RunnableParallel({\"answer\": answer, \"context\": itemgetter(\"context\")})`: the answer is the result of passing the prompt through the LLM, and the context is the one obtained in the first step\n",
    "\n",
    "Here, the input to the prompt is expected to be a dictionary with the keys “context” and “question”. The user input is just the question, so we need to get the context using our retriever and passthrough the user input under the “question” key.\n",
    "\n",
    "I'm not excessively proficient on LangChain so I'm sure there are better ways to write this chain to enhance readability. If you know of any potential improvements, feel free to let me know!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    1175.93 ms\n",
      "llama_print_timings:      sample time =      12.09 ms /    58 runs   (    0.21 ms per token,  4797.35 tokens per second)\n",
      "llama_print_timings: prompt eval time =   77701.33 ms /   687 tokens (  113.10 ms per token,     8.84 tokens per second)\n",
      "llama_print_timings:        eval time =   11026.84 ms /    57 runs   (  193.45 ms per token,     5.17 tokens per second)\n",
      "llama_print_timings:       total time =   88983.37 ms /   744 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'answer': '\\nAnswer: The Black Knives were a group of assassins who were rumored to be Numen who had close ties with Marika herself during the Night of the Black Knives. They were all women and were responsible for carrying out deeds during that eventful night.',\n",
       " 'context': [Document(page_content='Gauntlets used by the Black Blade Assassins. Gauntlets used by the Black Knife Assassins.\\r\\nCrafted with scale armor that makes no sound.\\r\\n\\r\\nThe assassins that carried out the deeds of the Night of the Black Knives were all women, and rumored to be Numen who had close ties with Marika herself.', metadata={'title': 'Black Knife Gauntlets', 'category': 'protector'}),\n",
       "  Document(page_content=\". Unique curved sword, notched like shark's teeth.\\r\\nWeapon carried by corpse pillagers who prowl the sites of old battles.\\r\\n\\r\\nThe blade is tacky with blood and covered in hefty nicks, making it totally uneven. Life can be sinister indeed.\", metadata={'title': \"Scavenger's Curved Sword\", 'category': 'weapon'}),\n",
       "  Document(page_content='Throw fanned-out knives at enemies to inflict damage. A set of five throwing knives bundled together.\\r\\nA concealed weapon cherished by the raptor assassins.\\r\\n\\r\\nThe thin knives fan out when thrown, dealing damage to the target.\\r\\n\\r\\nEach knife deals paltry damage, but the wide range makes it suitable for constraining enemies.', metadata={'title': 'Fan Daggers', 'category': 'goods'}),\n",
       "  Document(page_content=\"Mark of the Night of the Black Knives ritual. On the Night of the Black Knives, someone stole a fragment of Death from Maliketh, the Black Blade, and imbued its power into the assassins' daggers.\\r\\n\\r\\nThis mark is evidence of the ritual, and hides the truth of the conspiracy.\", metadata={'title': 'Black Knifeprint', 'category': 'goods'}),\n",
       "  Document(page_content='. Curved greatswords of black steel wielded by General Radahn.\\r\\nA pair of weapons decorated with a lion mane motif.\\r\\n\\r\\nRadahn earned considerable renown as the Starscourge in his youth, and it is said that it was during this time he engraved the gravity crest upon these blades.', metadata={'title': 'Starscourge Greatsword', 'category': 'weapon'})]}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = chain.invoke({\"question\": \"Who were the Black Knives?\"})\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at our output here:\n",
    "\n",
    "- Timings: these are not very interesting right now, but they can be useful when attempting to optimize your application. For example, this interaction had a total response time of over 20 seconds, which is probably not great for a production application.\n",
    "- Output: in this chain, I chose to output both the answer and the context so we can analyse the information that influenced the LLM's response.\n",
    "\n",
    "This is the answer an end user would get:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: The Black Knives were a group of assassins who carried out the Night of the Black Knives, a secretive event that occurred in the past.\n",
      "They were rumored to be Numen who had close ties with Marika herself.\n",
      "The assassins were all women and were known to be skilled in combat and stealth.\n",
      "They were also known to be equipped with unique weapons such as the Black Knife Gauntlets and Scavenger's Curved Sword.\n",
      "The Night of the Black Knives was a conspiracy that involved stealing a fragment of Death from Maliketh, the Black Blade, and imbuing its power into the assassins' daggers.\n",
      "The Mark of the Night of the Black Knives ritual was also performed on this night, and it is believed that this ritual hides the truth of the conspiracy.\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    response[\"answer\"].replace(\". \", \".\\n\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results are somewhat sensible, but take a closer look at the following statement:\n",
    "\n",
    "> They were also known to be equipped with unique weapons such as the Black Knife Gauntlets and Scavenger's Curved Sword.\n",
    "\n",
    "This mentions the *Scavenger's Curved Sword*. However, if you read the document from the context that originated this fragment, you will see that it has no relation to the Black Knives at all. In these cases, consider the following methods:\n",
    "\n",
    "- lower the `k` value used in similarity search to reduce the number of relevant results\n",
    "- change the `search_type` argument depending on your needs, since in the example there are many other documents containing the exact substring \"Black Knives\" that do not appear in the context due to using maximum marginal relevance, which penalizes redundancy\n",
    "\n",
    "This shows that domain knowledge may be useful when evaluating and debugging these systems. So, while we're on the topic:\n",
    "\n",
    "## Optimization and Advanced Features\n",
    "\n",
    "This is the very basics of a RAG architecture. There's not much to do with it while it's confined to a notebook, so here are a few ways you can take it a step further:\n",
    "\n",
    "- **Prompt optimization**: aside from fine-tuning your prompt manually, you can try letting the LLM write it for you! Automatically generated prompts[^1] have proven to perform slighly better than some hand-tuned prompts.\n",
    "- **Evaluation**: use the [RAG triad of measures](https://www.trulens.org/trulens_eval/getting_started/core_concepts/rag_triad/) to assess the performance of your application. One simple and straight-forward way to do it is to ask the LLM itself (or another one prepared for an evaluation task) to grade the generated response based on the provided context.\n",
    "- **Deployment**: so far, Streamlit is the easiest way I've found to interact with the system in a chat-like enviroment. Check out this [short guide](https://docs.streamlit.io/develop/tutorials/llms/build-conversational-apps).\n",
    "- **Scalability**: in a laptop or any other mid-tier machine, the response times of the LLM will probably be quite high. Consider using smaller models, or cloud instances for hosting your application in any of the commercial hyperscalers.\n",
    "\n",
    "[^1]: [Battle, R., & Gollapudi, T. (2024). The Unreasonable Effectiveness of Eccentric Automatic Prompts](https://arxiv.org/pdf/2402.10949.pdf)\n",
    "\n",
    "\n",
    "New and exciting stuff is coming to light every few days in this field, so keep an eye open and you're sure to find new improvements for your application. \n",
    "\n",
    "## Conclusion\n",
    "\n",
    "That's it! Your very own RAG application now lives and runs on your computer, as long as it doesn't spontaneously combust during the inference process.\n",
    "\n",
    "Feel free to test it with your own data as well, since LangChain offers different loader classes to read from webpages, PDF documents and the like. The advent of somewhat easily accesible LLMs opens up a myriad possibilities for new projects and ideas. It is a constantly evolving landscape and you can get creative with stuff like chatbots, research assistants, or content generators. The skills from this guide are fairly basic but building on them can take your NLP game to the next level.\n",
    "\n",
    "Now go forth and deploy something cool with RAG tech!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
