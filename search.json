[
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "RuneLog\n\n\nML Experiment Tracker\n\n\n\n\n\nDescription\n\nA simple, file-based Python library for tracking machine learning experiments, inspired by MLflow.  \n\nKey features:\n\n\n\n\nSimple, Zero-Overhead Setup\n\n\nLocal-first, Lightweight\n\n\nPortable and Transparent\n\n\n\nDocumentation Source code\n\n\n\n\n\n\n\nTrophy Hunter Dashboard\n\n\nStreamlit Dashboard for PSN Trophy Tracking\n\n\n\n\n\nDescription\n\nWeb-based dashboard for gamers who want to visualize and analyze their PlayStation trophy collections.  \n\nKey features:\n\n\n\n\nInstant Profile Summary\n\n\nIn-Depth Trophy Log Analysis\n\n\nRich Visualizations and Platinum Mosaic\n\n\n\nDocumentation Source code\n\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "posts/20191208-pint-of-api/index.html",
    "href": "posts/20191208-pint-of-api/index.html",
    "title": "A Pint of API",
    "section": "",
    "text": "API stands for Application Programming Interface. In other words, it’s an interface that makes it easier for developers to program different applications."
  },
  {
    "objectID": "posts/20191208-pint-of-api/index.html#definition",
    "href": "posts/20191208-pint-of-api/index.html#definition",
    "title": "A Pint of API",
    "section": "",
    "text": "API stands for Application Programming Interface. In other words, it’s an interface that makes it easier for developers to program different applications."
  },
  {
    "objectID": "posts/20191208-pint-of-api/index.html#some-examples",
    "href": "posts/20191208-pint-of-api/index.html#some-examples",
    "title": "A Pint of API",
    "section": "Some examples",
    "text": "Some examples\n\nTwitter\nBooking.com\nYour computer’s file system"
  },
  {
    "objectID": "posts/20191208-pint-of-api/index.html#restful-apis",
    "href": "posts/20191208-pint-of-api/index.html#restful-apis",
    "title": "A Pint of API",
    "section": "RESTful APIs",
    "text": "RESTful APIs\nThese are APIs that follow the REST architectural style. REST stands for REpresentational State Transfer, and it makes it easier for systems to communicate between one another. By using a REST interface, different clients can hit the same endpoints and perform the same actions to obtain the same responses.\nTherefore, the code that implements the server and the one that implements the client can be developed independently and without previous knowledge of each other. Changes to these codebases can be performed at any time without affecting the other codebase, as long as each side knows what format to expect in the data.\nSystems following this architectural paradigm are stateless, which means that the server and the client do not need to know anything about the state of their counterpart. This is enforced with the use of resources, which are any object, document, or element of the Web. REST systems interact through standard operations on these resources."
  },
  {
    "objectID": "posts/20191208-pint-of-api/index.html#communication",
    "href": "posts/20191208-pint-of-api/index.html#communication",
    "title": "A Pint of API",
    "section": "Communication",
    "text": "Communication\nIn the REST architecture, the client sends a request that retrieves or modifies a given resource, and the server sends a response to the request.\n\nRequests\nA request generally consists of:\n\nan HTTP verb defining the operation to perform\na header, containing metadata about the request\na path to a resource, also called an endpoint\nan optional message body with a data payload\n\n\n\n\nA Pint of API\n\n\n\nHTTP verbs\nThe basic verbs to interact with resources over HTTP are:\n\nGET — to retrieve a resource or collection of resources\nPOST — to create a new resource\nPUT — to update a resource\nDELETE — can you guess?\n\n\n\nEndpoints\nAn endpoint is a path that points to a resource. These are normally hierarchical and tend to be quite descriptive, so that the client can understand what resource they are accessing even if they have never seen that specifiv path before.\nFor example, take https://api.github.com/users/gonz4lex/repos. This points to the repos endpoint, which stores information about the repositories of the user with the name gonz4lex (that’s me!), and is used, for instance, in a GET request to retrieve information about the specific resource.\n\n\n\nResponses\nResponses from the server generally contain the data payload requested by the client as well as response metadata, such as status code and content type.\n\nResponse codes\nResponses contain information of the success status of the operation, to inform the client of its success (or failure). There are many such codes, but it’s only important to know the most common ones and learn how they can be used.\n\n\n\n\n\n\n\nStatus code\nMeaning\n\n\n\n\n200 (OK)\nSuccesful standard HTTP request.\n\n\n201 (CREATED)\nSuccesful request that results in the creation of an item.\n\n\n204 (NO CONTENT)\nSuccesful request that returns an empty response body.\n\n\n400 (BAD REQUEST)\nUnsuccesful request due to client error such as bad syntax\n\n\n403 (FORBIDDEN)\nUnsuccesful request due to lack of authentication or credentials.\n\n\n404 (NOT FOUND)\nUnsuccesful request because the resource could not be found.\n\n\n500 (INTERNAL SERVER ERROR)\nGeneric response to unexpected failure when no futher info is available."
  },
  {
    "objectID": "posts/20191208-pint-of-api/index.html#references",
    "href": "posts/20191208-pint-of-api/index.html#references",
    "title": "A Pint of API",
    "section": "References",
    "text": "References\n[1] https://www.codecademy.com/articles/what-is-rest"
  },
  {
    "objectID": "posts/20191126-vs-code-ninja/index.html",
    "href": "posts/20191126-vs-code-ninja/index.html",
    "title": "Master of Visual Studio Code",
    "section": "",
    "text": "Visual Studio Code is a lightweight code editor with a lot of powerful capabilities. If you like Visual Studio, this is quite similar but without the abhorrent load times. It’s available for the main operative systems and includes built-in support for JavaScript. It has a rich ecosystem of extensions that extend support to other languages such as C#, Java, Python, and SQL, as well as runtimes like .NET and Unity.\nIt ships with great features such as IntelliSense, debugging and in-product source control."
  },
  {
    "objectID": "posts/20191126-vs-code-ninja/index.html#introduction",
    "href": "posts/20191126-vs-code-ninja/index.html#introduction",
    "title": "Master of Visual Studio Code",
    "section": "",
    "text": "Visual Studio Code is a lightweight code editor with a lot of powerful capabilities. If you like Visual Studio, this is quite similar but without the abhorrent load times. It’s available for the main operative systems and includes built-in support for JavaScript. It has a rich ecosystem of extensions that extend support to other languages such as C#, Java, Python, and SQL, as well as runtimes like .NET and Unity.\nIt ships with great features such as IntelliSense, debugging and in-product source control."
  },
  {
    "objectID": "posts/20191126-vs-code-ninja/index.html#explorer",
    "href": "posts/20191126-vs-code-ninja/index.html#explorer",
    "title": "Master of Visual Studio Code",
    "section": "Explorer",
    "text": "Explorer\nUse Ctrl+B to toggle the sidebar, where you can find the main functionalities. In Code, you work in directories or folders where your source code lives. The explorer allows for quick access and manipulation of the files in your project. You can use Ctrl+P to open files in your folders faster."
  },
  {
    "objectID": "posts/20191126-vs-code-ninja/index.html#search",
    "href": "posts/20191126-vs-code-ninja/index.html#search",
    "title": "Master of Visual Studio Code",
    "section": "Search",
    "text": "Search\nSearch files and inside of them. Match whole words or substrings, toggle case-sensitive search, use regex, and filter for different file types. You can launch the search and replace feature from this pane, as well as by using Ctrl+F."
  },
  {
    "objectID": "posts/20191126-vs-code-ninja/index.html#source-control",
    "href": "posts/20191126-vs-code-ninja/index.html#source-control",
    "title": "Master of Visual Studio Code",
    "section": "Source control",
    "text": "Source control\nYou can integrate with both Git and Microsoft’s TFVC (Team Foundation Version Control), although the latter needs to be installed. By opening a folder that’s mapped to a TFVC repository, you can automatically check folders in and out, write your commit messages, view your work items or tasks and associate commits to them, all from inside the editor."
  },
  {
    "objectID": "posts/20191126-vs-code-ninja/index.html#debugger",
    "href": "posts/20191126-vs-code-ninja/index.html#debugger",
    "title": "Master of Visual Studio Code",
    "section": "Debugger",
    "text": "Debugger\nLaunch the debugger panel by pressing Ctrl+Shift+D and click the little configuration icon to create a debugging task. The editor detects your debugging environment automatically, but if this fails you can manually choose it. Next, you can set breakpoints wherever you want your program to stop and press F5 to start debugging. Now, you can inspect the status of all your variables and functions, examine their values and attributes, at any point in your file. In the panel you will see four main areas:\n\nVariables: contains all of the existing variables in your program.\nWatch: this contains those variables that you have flagged for further inspection, so that they are easy to find.\nCall Stack: this explains how the program got to where it currently is.\nBreakpoints: here you can see any breakpoints that you have set and quickly refer to them to disable, enable, or reapply them.\n\nBreakpoints are set by clicking on the gutter of your file or by pressing F9 and they stop the execution of your program. You can change a breakpoint to also be a log points, that is, some info will be logged to the console when the program execution reaches that point.\nWhen debugging, you can take a few actions that are know as “stepping through code”:\n\nStep over: this will step over a given line, meaning that if the line contains a function, it will be executed and the result returned without debugging each line.\nStep into: if the line you are going to step into contains a function, the debugger will enter it and continue working line by line. Otherwise it will behave as step over.\nStep out: this takes the debugger to the line where the current function was called.\n\nYou can also launch an interactive debugging console with Ctrl+Shift+Y.\nThis is just a subset of the debugging capabilities of VS Code, and the topic deserves a course on its own."
  },
  {
    "objectID": "posts/20191126-vs-code-ninja/index.html#extensions",
    "href": "posts/20191126-vs-code-ninja/index.html#extensions",
    "title": "Master of Visual Studio Code",
    "section": "Extensions",
    "text": "Extensions\nOne of the killer features of Code. The Visual Studio marketplace is community-sourced and you can make your own extensions with JavaScript or TypeScript. There are loads of useful features in extensions, but take into account that every extension that you install with have a small impact in the performance of your editor. Also, they bloat the %APPDATA% folder in your roaming profile, making your machine start up a lot more slowly.\nAs a workaround, you can write a one line batch file that loads the editor’s metadata from a location outside the roaming profile where you have previously placed the Code folder.\nFor example, my setup is as follows:\ncode --extensions-dir \"C:/Code\" --user-data-dir \"C:/Code\"\nSave this with a .bat extension and place it in your desktop. Next time you open VS Code, you can do it from that icon. It’s not the most elegant solution, but it works in corporate machines.\nAlternatively, check the portable version of VS Code.\nThey can also be enabled or disabled whenever necessary at the user or the workspace level."
  },
  {
    "objectID": "posts/20191126-vs-code-ninja/index.html#integrated-terminal",
    "href": "posts/20191126-vs-code-ninja/index.html#integrated-terminal",
    "title": "Master of Visual Studio Code",
    "section": "Integrated Terminal",
    "text": "Integrated Terminal\nYou can open several terminals in different languages, stack them on top of each other or put them to the side. You can use Ctrl+` (or Ctrl+Ñ in my Spanish keyboard) to toggle it."
  },
  {
    "objectID": "posts/20191126-vs-code-ninja/index.html#the-command-palette",
    "href": "posts/20191126-vs-code-ninja/index.html#the-command-palette",
    "title": "Master of Visual Studio Code",
    "section": "The command palette",
    "text": "The command palette\nPress Ctrl+Shift+P to launch the palette. This is, by far, the most important keyboard shortcut in VS Code because it allows access to every existing command in the editor. Just launch it, and start typing what you want to do."
  },
  {
    "objectID": "posts/20191126-vs-code-ninja/index.html#customization",
    "href": "posts/20191126-vs-code-ninja/index.html#customization",
    "title": "Master of Visual Studio Code",
    "section": "Customization",
    "text": "Customization\n\nThemes and icons\nThere are many, many different themes for VS Code and you can make your own ones as well. This is very useful when programming for long stretches of time, or when light reflects against your monitors. The keyboard command to change themes is Ctrl+K, Ctrl+T and is obviously one of my most used ones! Some of the themes I personally like:\n\n\n\nLight\nDark\n\n\n\n\nWinter is Coming\n1984\n\n\nSlack Theme\nMonokai\n\n\nQuiet Light\nFairyfloss\n\n\nAyu\nSynthwave\n\n\nNoctis\nNord\n\n\n\nYou can also use different icon themes, which make your files easier to find in the explorer pane while making it more appealing to look at! You can go to Preferences -&gt; File Icon Theme to change it, or of course use the all-powerful command palette.\nSome of my favorites are:\n\nMaterial Icons\n\nVS Code Icons\n\nAyu Icons (bundled with the Ayu theme)\n\n\nYou can also find an extension that changes your file icons and instead shows Nicolas Cage’s face.\nPlease use at your own risk.\n\nEvery one of these changes and customizations are saved to the user preferences, which are reachable via Ctrl+, or the command palette. You can view them in a nice UI or in JSON format. For example, you can see that I have custom setting for workbench.colorTheme and workbench.iconTheme. I also have window.zoomLevel set to -1 because I zoomed out with Ctrl+- Note that settings can be applied globally at the user level or relative to a workspace.\nSome configuration options that I think are quite nice and improve collaboration and development rythm:\n\n\n\n\n\n\n\nOption\nDescription\n\n\n\n\n\"editor.minimap.enabled\": false\nRemoves the minimap shown at the right side of the editor.\n\n\n\"explorer.confirmDelete\": false\nDeactivates the warning before deleting a file.\n\n\n\"explorer.confirmDragAndDrop\": false\nRemoves warning when moving files in the explorer.\n\n\n\"editor.formatOnSave\": true\nAutomatically formats your code on save event.\n\n\n\"editor.formatOnPaste\": true\nAutomatically formats your code when you paste it.\n\n\n\"javascript.format.enable\": true\nEnables a JS formatter, such as Prettier.\n\n\n\"editor.multiCursorModifier\": \"alt\"\nSelection of multiple line when holding Alt and clicking.\n\n\n\"files.trimTrailingWhitespace\": true\nDeletes whitespace in files.\n\n\n\"editor.quickSuggestionsDelay\": 0\nShow code actions without delay.\n\n\n\"tfvc.location\": \"path\\\\to\\\\tf.exe\"\nTell Code where to find the Team Foundation executable.\n\n\n\"powermode.enabled\": true\nOnly if you’re really serious about your coding.\n\n\n\nThere are much more settings that you can tweak, and each extension or language has their own settings. Change, mix and swap them to find the setup that works best for you!\n\n\nFonts\nYou can also use a custom font if you wish to. Make sure the .ttf file is installed in your system and set your preferred font in the settings. I (and many other developers) use Fira Code because it has nice ligatures and doesn’t cost $200."
  },
  {
    "objectID": "posts/20191126-vs-code-ninja/index.html#workspaces",
    "href": "posts/20191126-vs-code-ninja/index.html#workspaces",
    "title": "Master of Visual Studio Code",
    "section": "Workspaces",
    "text": "Workspaces\nA VS Code workspace is not a folder, or a working directory. It is a list of a project’s folders and files and can contain multiple working directories. You can save settings or disable extensions at the workspace level and they take precedence over the user settings. They are a bit like Visual Studio’s solutions.\nYou can create one of these workspaces by going to File -&gt; Save Workspace as..., or just type workspace in the command palette to see all the available options."
  },
  {
    "objectID": "posts/20191126-vs-code-ninja/index.html#editing-evolved",
    "href": "posts/20191126-vs-code-ninja/index.html#editing-evolved",
    "title": "Master of Visual Studio Code",
    "section": "Editing evolved",
    "text": "Editing evolved\n\nIntelliSense\nIntelliSense is awesome, and its capabilities get augmented by Microsoft’s IntelliCode extension which leverages AI to improve code completion.\nIn summary, the service provides code completion based on language semantics and your source code. The suggestions (variables, functions, methods…) pop up as you type and get filtered as you continue typing. You can bring up IntelliSense in any editor window by pressing Ctrl+Space or by typing a trigger character such as the dot character (.) for JavaScript class methods.\nYou can also see quick info on each of the suggestions, such as the definition of a variable or function, the expected parameters for a function or the documentation of a class or object.\n\n\n\n\n\n\n\n\n\n\n\nTypes of IntelliSense Completion. From https://code.visualstudio.com/docs/editor/intellisense.\n\n\n\n\n\nCode formatting and navigation\nAside from formatting on paste and save as mentioned in the settings section, you can also use the commands Format Document or Format Selection whenever needed, from the command palette or by right clicking the editor window.\nNavigating your files is much faster and easier with Code, for example: * You can jump directly to the definition of any function or variable, no matter where it is in your codebase. * You can also peek at its definition, maintaining your current position in the document. * You can also rename every occurrence of a symbol just by pressing F2 with the cursor on the symbol.\n\n\nErrors, warnings and info\nYou can find these in the problems tab, which can be activated with Ctrl+Shift+M. These problems are usually underlined in your code in different colors depending on the problem level. You can loop through them by pressing F8 or Shift+F8 which will also provide an inline window with details about the problem.\n\n\nKeyboard shortcuts\nThese are a very nice productivity aid that help developers avoid moving their hands back and forth between the keyboard and the mouse. While it might seem a negligible time, it’s quite common to do several times every hour and it accumulates towards the end of the day.\nThere are a lot of keyboards shortcuts and its quite impossible to remember them all. Thankfully, there is a very handy reference and you can remap your shortcuts very flexibly to better match your work process.\nSome of my recommendations and most used keyboard shortcuts:\n\n\n\n\n\n\n\nShortcut\nDescription\n\n\n\n\nCtrl+Shift+N\nOpen a new Code instance. Use it when launching from custom folder.\n\n\nCtrl+Shift+Tab\nShows you active files in the editor.\n\n\nShift+Alt+↑/↓\nCopy current line above/below.\n\n\nAlt+Click\nInsert cursor.\n\n\nShift+Alt+I\nInsert cursor at the end of each selected line.\n\n\nShift+Alt+(drag mouse)\nColumn or box selection.\n\n\nCtrl+G\nOpens the command palette to let you enter a line number to go to.\n\n\nCtrl+Shift+O\nShows the list of symbols found in the current file.\n\n\nAlt+-&gt;/Alt+&lt;-\nSwitch between open files.\n\n\nAlt+Ç\nToggle line comment.\n\n\nCtrl+K, Ctrl+T\nChange color theme.\n\n\nCtrl+X / Ctrl+C\nCut or copy entire line, if you have empty selection.\n\n\nCtrl+Shift+/\nJump to matching bracket.\n\n\nShift+Alt+F / Ctrl+K, Ctrl+F\nFormat document or selection.\n\n\nF12\nGo to symbol definition.\n\n\nCtrl+\\\nSplit editor.\n\n\nShift+Alt+0\nToggle editor layout between horizontal or vertical.\n\n\nCtrl+K, Z\nEnter Zen mode. Double press Esc to exit.\n\n\nCtrl+´\nShow the terminal. Ctrl+Ñ in Spanish keyboards.\n\n\nCtrl+K, P\nCopy path of the active file.\n\n\nCtrl+K, R\nReveal the active file in the system explorer.\n\n\n\nThis is just a very small subset of commands, and only for the base VS Code. Extensions add new features which can also be mapped to any shortcut keymap.\n\nNote that some shortcuts have different key chords. Ctrl+K, Ctrl+T is not the same as Ctrl+K, T. The latter means that you need to press the second key while Ctrl is not pressed down.\n\n\n\nCode snippets\nThis is another very cool feature not unique to Code, and many languages already come with ready-made snippets for you to use. You can also, of course, author your own snippets. Just type “snippet” in the command palette and you will see available options. You can set a snippets file for each language or a common file for a given user, with snippets in many languages.\nSome examples of useful snippets would be a TRY-CATCH block in T-SQL or a for-loop in JavaScript."
  },
  {
    "objectID": "posts/20191126-vs-code-ninja/index.html#extensions-showcase",
    "href": "posts/20191126-vs-code-ninja/index.html#extensions-showcase",
    "title": "Master of Visual Studio Code",
    "section": "Extensions showcase",
    "text": "Extensions showcase\n\n\n\n\n\n\n\nExtension\nDescription\n\n\n\n\nAzure Repos\nConnectivity to Azure Repos and Team Foundation Server.\n\n\nBetter Comments\nImproved comment functionality.\n\n\nBracket Pair Colorizer\nBetter code readability.\n\n\nCode Time\nMeasures your coding frequency, volume and speed.\n\n\nESLint\nJavaScript linting, needs ESLint library installed.\n\n\nGitLens\nSupercharged version control, Git only.\n\n\nLive Share\nRemote collaboration within VS Code.\n\n\nSettings Sync\nSynchronize settings across different machines.\n\n\nPolacode\nTake beautiful screenshots of your code.\n\n\nPrettier\nAutomatically format your files.\n\n\nSQL Server\nIntegration with Microsoft’s SQL Server.\n\n\nVisual Studio IntelliCode\nAI powered IntelliSense.\n\n\n\nYou can also get recommended extensions based on your usage of the editor. For example, I get recommendations for a Markdown linter because I often use that file extension, and for the Notepad++ keymaps because I have that editor installed."
  },
  {
    "objectID": "posts/20191206-power-bi-intro/index.html",
    "href": "posts/20191206-power-bi-intro/index.html",
    "title": "An introduction to Power BI for data scientists",
    "section": "",
    "text": "Get started with Power BI to speed up your exploratory data analysis when looking for trends and features than might be useful for prediction or classification with machine learning algorithms."
  },
  {
    "objectID": "posts/20191206-power-bi-intro/index.html#getting-data",
    "href": "posts/20191206-power-bi-intro/index.html#getting-data",
    "title": "An introduction to Power BI for data scientists",
    "section": "Getting Data",
    "text": "Getting Data\nConnecting to a data source in Power BI is quite straightforward. Just navigate to the Home tab in the ribbon and select Get Data to find a dropdown menu of possible options. Common sources are SQL Server, Analysis Services (SSAS), Excel and Dynamics 365, among others. In most cases, such as SQL Server, you can also specify the query you want to run against the database. Note that when connecting to a source you can either query the data directly through DirectQuery or Import modes, but you cannot combine both methods into the same solution. There is also the LiveConnection mode, a variation of DirectQuery geared towards SSAS Tabular that we will not cover here.\nThere are some crucial differences between the two connection modes: * In DirectQuery, the data stays at the source and is not imported into your machine’s memory and the size of the .pbix file is dramatically lower than an import solution. However, each event during report visualization will trigger live queries that might harm the user experience. * In Import mode, the data model is loaded into Power BI desktop and remains static until a refresh is triggered. This makes for smooth user experience but increase file size and memory consumption.\nWhen you have selected the source, Power BI will present you with a preview of your data. If everything looks OK, you can continue towards the next step. Otherwise, you can review your query and make any necessary changes.\nYou can also use a folder as your source, allowing you to import several files at once such as a series of Excel or CSV files. They are loaded as binary data types and double clicking them loads their values."
  },
  {
    "objectID": "posts/20191206-power-bi-intro/index.html#clean-and-transform",
    "href": "posts/20191206-power-bi-intro/index.html#clean-and-transform",
    "title": "An introduction to Power BI for data scientists",
    "section": "Clean and Transform",
    "text": "Clean and Transform"
  },
  {
    "objectID": "posts/20191206-power-bi-intro/index.html#data-modeling",
    "href": "posts/20191206-power-bi-intro/index.html#data-modeling",
    "title": "An introduction to Power BI for data scientists",
    "section": "Data Modeling",
    "text": "Data Modeling\nThere some key aspects to take into account when modeling data with Power BI, such as data categories and summarization; or establishing adequate relationships among your tables. You can also create custom hierarchies such as Year/Month/Week or Country/Destination/Region by dragging dragging columns on top of each other in the Report or Data View.\n\nData types, categories and format strings\nThere are several different types of data to work with in Power BI, such as integer, decimal/fixed, currency, date/time/datetime, string and binary objects. The data type is what allows certain functions and operations to be performed on a specific column, and it’s not the same as the data format.\nThe format string is the way that this data will be visually represented, such as percentage, with thousand separator, with decimals… There also many different formats for date and time objects which can also be changed flexibly with the DAX FORMAT ( ) function.\nText data can be categorized, which is useful when you have geographical data or URLs in your table. Setting a data category for these columns allows for advanced features such as URL icons, embedded images and map visuals. Numerical data is uncategorized by default, since we use data types to differentiate it. Both types of data need summarization, which is the default statistic that will be calculated by visuals using the measure. Possible options are sum, average, minimum, maximum, count and distinct for numerical types, while text types only have count and distinct. You can also use first and last within some visuals.\nIt’s important to make a distinction between modeling your data within the Power Query editor or within Power BI Desktop itself, since different operations can be performed and the computational load is different. Usually, we use the Query Editor to clean, shape and transform data from the source, while the Data View (i.e. Power Pivot) is often used to model the imported data and define calculations for analytics.\n\n\nPower Query editor and the M language\nPower Query is a data discovery and query tool, good for shaping and mashing up data even before you’ve added the first row of data to the table. Inside the Power Query editor you can use the M programming language to shape your data, creating step by step scripts that iteratively clean, transform and add columns to your model. Moreover, you can leverage the amazing Power BI user interface that makes this data engineering process very straight-forward and with zero necessary knowledge of M.\nSome of the operations you can perform are:\n\nDuplicate, transform or split columns\n\nYou can use regular expressions, functions and calculations to engineer new features.\n\nChange data types and formats\n\nPower Query can also detect automatically the best data type for your column if you are not sure.\n\nPivot, unpivot and sort columns\nMerge (join) and append (union) queries\nRemove or keep duplicates, errors, nulls, headers…\nGroup by columns and aggregate measures\n\n\n\nEnhance your model with the Data View\nNot only can you explore all of your data interactively within this view, but you can also further improve the analytical capabilities of your model by using DAX expressions that leverage different filtering contexts and time frames to create complex features.\n\n\nCreate calculated columns\nYou can create calculated columns, for example, to establish a relationship between different tables, or to create indexes that could later be used as reference by iterator functions. Creating a calculated column consumes memory in your machine as it expands your data model if your tables have many records. Consider creating them only when necessary or for extremely complex calculations that should be run live.\nRemember to custom sort your columns, i.e. MonthName should be sorted by MonthNumber and not alphabetically by itself.\n\n\nCreate calculated measures\nMeasures are calculations that exists in your Power BI data model and is computed on the fly at each event. With DAX you can define a measure once and then slice it by all the different fields in your model, as long as the necessary relationships exist.\nSince they are computed at query time, calculated measures consume more CPU than a column, but they do not grow the tables in your data model.\n\n\nCreate calculated tables\nCalculated tables allow you to express a whole new range of modeling capabilities, such as performing different types of joins and creating new tables on the fly based on the results of a formula. For example, you can create a date dimension with DAX by using the date to derive all necessary attributes such as month, week number or day of the week, or create summary tables with aggregated values.\n\n\nRelationships in the data model\nData relationships are similar to how a cube is structured in Analysis Services. In the Model View you can access a diagrammatic view of your data, and you can define how your tables are related. To create a relationship, just click on a column on your fact table and drag it into the key on the dimension table. This generates an arrow indicating the active relationship, which you can double click to access advanced options such as setting the cardinality or the direction of the relationship.\nYour options for cardinality are Many to One, and One to One. Many to One is the fact to dimension type relationship, for example a sales table with multiple rows per product being matched up with a table listing products in their own unique row."
  },
  {
    "objectID": "posts/20191206-power-bi-intro/index.html#visualization-and-report-design",
    "href": "posts/20191206-power-bi-intro/index.html#visualization-and-report-design",
    "title": "An introduction to Power BI for data scientists",
    "section": "Visualization and report design",
    "text": "Visualization and report design\nVisuals are the main element of Power BI reporting, and they have a wide range of formatting options that allows for almost limitless customization.\n\nEditing interactions\nWhen you have multiple visualizations on the same report page, selecting a particular segment by clicking or using a slicer will affect all the visuals on that page. In some cases, though, you may want to slice only specific visuals. For example, you might not want a scatter plot to be cross-filtered because that would remove crucial meaning from the visualization. You can define how visuals interacts with each other when the user selects data points.\n\n\nZ-order\nWhen you have lots of elements on a report, Power BI lets you manage how they overlap with each other. How items are layered, or arranged on top of one another, is often referred to as the z-order. This is particularly useful when you want to use shapes to create backgrounds, highlights or containers. For example, you might use a shape to create the background for your report title, or to enclose a set of KPIs in a container. You can also set a semi-transparent shape over a graph section to highlight it.\n\n\nBookmarks\nBookmarks are a great feature that allow for some nice tricks with user interaction. Essentially, a bookmark is a snapshot of your report page at a given time. They can save filter state, formatting options, visual placement, etc. You can use them to change between report pages, create a guided navigation for the user, change themes, show help or filter panes, send emails and many more things."
  },
  {
    "objectID": "posts/20191206-power-bi-intro/index.html#time-intelligence",
    "href": "posts/20191206-power-bi-intro/index.html#time-intelligence",
    "title": "An introduction to Power BI for data scientists",
    "section": "Time Intelligence",
    "text": "Time Intelligence\nTime intelligence manages complex time-based calculations with reduced user input. It needs a date table with no date gaps, which Power BI creates automagically if the setting Auto Date/Time is enabled, which is the default case. It is recommended to create your own date table instead of relying on this one, since it’s not possible to adjust if necessary and can actually only be viewed from DAX Studio.\nIf you decide to work with it, you can use dot notation to access the time calculations, i.e. Orders[Date].[Year] to automatically extract the year of that date."
  },
  {
    "objectID": "posts/20220423-github-comment-system/index.html",
    "href": "posts/20220423-github-comment-system/index.html",
    "title": "A comment system for static websites using Github Discussions",
    "section": "",
    "text": "I’ve been searching for an easy alternative to a comment system for this blog, which is built with Pelican and served through Github Pages. Being a static website, traditional server-based solutions are not a good fit or they require too much setup work that I really didn’t want to do. So after many months procrastinating on this, I finally sat down and got it work in less than an hour. Easy!"
  },
  {
    "objectID": "posts/20220423-github-comment-system/index.html#setting-up",
    "href": "posts/20220423-github-comment-system/index.html#setting-up",
    "title": "A comment system for static websites using Github Discussions",
    "section": "Setting up",
    "text": "Setting up\nI looked mainly at two solutions, utterances (based on Github issues) and giscus which is still under active development since it’s based on a newer feature, Github Discussions. So naturally, I chose the more unstable and experimental solution.\nThe setup is extremely straight-forward and can be found either on the app website or the repo, so make sure to check those sources as well.\nStart by heading to https://giscus.app to begin the process."
  },
  {
    "objectID": "posts/20220423-github-comment-system/index.html#make-your-website-giscus-ready",
    "href": "posts/20220423-github-comment-system/index.html#make-your-website-giscus-ready",
    "title": "A comment system for static websites using Github Discussions",
    "section": "Make your website giscus-ready",
    "text": "Make your website giscus-ready\nYour website’s repo needs to be on Github, for obvious reasons, and needs:\n\npublic access, so discussions can be viewed\nthe giscus app installed\nthe Discussions feature turned on\n\nYou can simply follow the guidelines in the website to generate you definitive script tag which just needs to be plugged into your HTML template, and it looks like this:\n&lt;script src=\"https://giscus.app/client.js\"\n        data-repo=\"[ENTER REPO HERE]\"\n        data-repo-id=\"[ENTER REPO ID HERE]\"\n        data-category=\"[ENTER CATEGORY NAME HERE]\"\n        data-category-id=\"[ENTER CATEGORY ID HERE]\"\n        data-mapping=\"pathname\"\n        data-reactions-enabled=\"1\"\n        data-emit-metadata=\"0\"\n        data-input-position=\"bottom\"\n        data-theme=\"light\"\n        data-lang=\"en\"\n        crossorigin=\"anonymous\"\n        async&gt;\n&lt;/script&gt;"
  },
  {
    "objectID": "posts/20220423-github-comment-system/index.html#plug-and-play",
    "href": "posts/20220423-github-comment-system/index.html#plug-and-play",
    "title": "A comment system for static websites using Github Discussions",
    "section": "Plug-and-play",
    "text": "Plug-and-play\nThe only necessary step left is to go into your desired template (article.html in my case) and just paste that snippet of code wherever I want my comments to show up:\n&lt;!-- article.html --&gt;\n  ...\n  &lt;div class=\"article_text\"&gt;\n    {{ article.content }}\n  &lt;/div&gt;\n  &lt;div class=\"article_meta\"&gt;\n      ...\n  &lt;/div&gt;\n\n  {% if GISCUS_ENABLED %}\n  &lt;div id=\"article_comments\"&gt;\n    &lt;div id=\"comment_thread\"&gt;\n    &lt;/div&gt;\n    &lt;script src=\"https://giscus.app/client.js\" data-repo=\"gonz4lex/gonz4lex.github.io\"\n    data-repo-id=\"MDEwOlJlcG9zaXRvcnkyMjU5MTg3MDM=\" data-category=\"Announcements\"\n    data-category-id=\"DIC_kwDODXc-784COr2Q\" data-mapping=\"pathname\" data-reactions-enabled=\"1\" data-emit-metadata=\"0\"\n    data-input-position=\"bottom\" data-theme=\"light\" data-lang=\"en\" data-loading=\"lazy\" crossorigin=\"anonymous\" async&gt;\n      &lt;/script&gt;\n  &lt;/div&gt;\n  {% endif %}\nI added an option GISCUS_ENABLED flag on my settings file, but that’s entirely optional.\nAfter that, just reload and regenerate your site, and comments should show up on your posts!"
  },
  {
    "objectID": "posts/20220423-github-comment-system/index.html#some-closing-thoughts",
    "href": "posts/20220423-github-comment-system/index.html#some-closing-thoughts",
    "title": "A comment system for static websites using Github Discussions",
    "section": "Some closing thoughts",
    "text": "Some closing thoughts\nEven if this isn’t a big change or project, it’s another reminder of why it’s almost always better to dive right away into demos or proofs of concepts rather than getting stuck on an endless loop of analysis paralysis. If I hadn’t waited so long because I wasn’t sure how to best tackle this feature, I would had probably finished months ago.\nHowever, before implementing this feature yourself, make sure it works by trying out the comment section below!"
  },
  {
    "objectID": "posts/20240402-rag-system-with-langchain/rag-system.html",
    "href": "posts/20240402-rag-system-with-langchain/rag-system.html",
    "title": "Understanding RAG systems with a practical application using locally executed LLMs",
    "section": "",
    "text": "One of the latest trending methods in modern NLP is Retrieval Augmented Generation (RAG), which is a recent approach to improve the information retrieval and generation capabilities of large language models. In a RAG system, the models are used to extract relevant information from large datasets and generate coherent responses and insights from them. This is a invaluable technique for several common use cases such as question answering or conversational agents. However, cutting-edge LLMs take vast amount of resources for training and inference and are not usually free to access."
  },
  {
    "objectID": "posts/20240402-rag-system-with-langchain/rag-system.html#introduction",
    "href": "posts/20240402-rag-system-with-langchain/rag-system.html#introduction",
    "title": "Understanding RAG systems with a practical application using locally executed LLMs",
    "section": "",
    "text": "One of the latest trending methods in modern NLP is Retrieval Augmented Generation (RAG), which is a recent approach to improve the information retrieval and generation capabilities of large language models. In a RAG system, the models are used to extract relevant information from large datasets and generate coherent responses and insights from them. This is a invaluable technique for several common use cases such as question answering or conversational agents. However, cutting-edge LLMs take vast amount of resources for training and inference and are not usually free to access."
  },
  {
    "objectID": "posts/20240402-rag-system-with-langchain/rag-system.html#motivation",
    "href": "posts/20240402-rag-system-with-langchain/rag-system.html#motivation",
    "title": "Understanding RAG systems with a practical application using locally executed LLMs",
    "section": "Motivation",
    "text": "Motivation\nOne of the most typical applications of RAG is question answering over corporate documents, such as company guidelines, product reviews, or documentation. While these applications are common, I prefer to explore more unconventional data sources that may lead to more engaging projects.\nIn this article, we will walk through the process of running an LLM locally and using it to build our RAG application. This application will consist of a simple chatbot that can retrieve context from a knowledge base containing documents related to the award-winning videogame Elden Ring.\n\nBackground: Elden Ring\nElden Ring is renowned for its intricate and immersive story, with narrative elements such as environmental storytelling, lore and mythos found in item, skill and equipment descriptions; and NPC interactions. Much of this story is left open to the player’s interpretation of cryptic dialogue and obscure descriptions, as well as drawing connections between disparate elements to create a cohesive narrative of the world.\nWith all that preamble out of the way, let’s create a chatbot that serves as an expert in the lore of Elden Ring and can assist the user in exploring the different connections in the game’s rich storytelling."
  },
  {
    "objectID": "posts/20240402-rag-system-with-langchain/rag-system.html#a-primer-on-rag-systems",
    "href": "posts/20240402-rag-system-with-langchain/rag-system.html#a-primer-on-rag-systems",
    "title": "Understanding RAG systems with a practical application using locally executed LLMs",
    "section": "A Primer on RAG Systems",
    "text": "A Primer on RAG Systems\n\n\n\n\n\n\nNote\n\n\n\nIf you want to skip the theory and preprocessing, go straight here.\n\n\nRAG systems combine the strengths of infomation retrieval and natural language to effectively understand and generate human-like responses. Not unexpectedly, the two key components of such systems are retrieval and generation.\n\nRetrieval\nIn the retrieval step, the system searches its knowledge base for hits matching a user’s query. The knowledge base should be stored in a vector database with embeddings for efficient search with techniques such as semantic similarity measures. The goal of this step is to extract the most relevant pieces of information given the original prompt, and those that could help in generating a coherent response. Once the context information is collected, the generation step can use it for further processing.\n\n\nGeneration\nIn the generation step, the system feeds the obtained context to an LLM, such as GPT or BERT, that can generate coherent output based on the provided information. Coupling the natural language understanding of the model with the context results in responses that are gramatically correct, contextually relevant and coherent."
  },
  {
    "objectID": "posts/20240402-rag-system-with-langchain/rag-system.html#application-design",
    "href": "posts/20240402-rag-system-with-langchain/rag-system.html#application-design",
    "title": "Understanding RAG systems with a practical application using locally executed LLMs",
    "section": "Application Design",
    "text": "Application Design\nBefore writing the first line of code, let us take some time to design our application and define its scope."
  },
  {
    "objectID": "posts/20240402-rag-system-with-langchain/rag-system.html#development-environment-setup",
    "href": "posts/20240402-rag-system-with-langchain/rag-system.html#development-environment-setup",
    "title": "Understanding RAG systems with a practical application using locally executed LLMs",
    "section": "Development Environment Setup",
    "text": "Development Environment Setup\n\n\n\n\n\n\nWarning\n\n\n\nLLM inference needs a lot of computing power, so make sure your machine can handle it before continuing. For reference, I ran this on a Windows laptop with an 11th Gen i7 chip and 32GB RAM. If you have better or similar specs (or GPU) in your computer, you are probably good to go.\n\n\nYou will need the following stuff in order to run the code in this guide:\n\nLangChain: a Python library that provides a framework for building LLM-powered applications\na large language model file: TheBloke’s HuggingFace page already has many models that are already quantized, lifting some of the process from your machine. Use the 7B or 13B parameter models depending on your system. More on this further below.\n\nThen, let’s create a virtual environment with the necessary requirements:\npython -m venv .venv\nsource .venv/bin/activate\npip install langchain docarray pandas"
  },
  {
    "objectID": "posts/20240402-rag-system-with-langchain/rag-system.html#prepare-the-knowledge-base",
    "href": "posts/20240402-rag-system-with-langchain/rag-system.html#prepare-the-knowledge-base",
    "title": "Understanding RAG systems with a practical application using locally executed LLMs",
    "section": "Prepare the Knowledge Base",
    "text": "Prepare the Knowledge Base\nIn order to feed your LLM of choice with the required context, you will need a knowledge base that can be parsed and incorporated into a vector database. For this guide, I will use the data available from the excellent Elden Ring Explorer project, which in turn comes from the Carian Archive.\n\nimport re\nimport json\nimport requests\nimport pandas as pd\n\n\ndata_url = \"https://eldenringexplorer.github.io/EldenRingTextExplorer/elden_ring_text.json\"\nresponse = requests.get(data_url)\n\ndata = response.json()\n\n\n# Peek at the data format\nfor key1 in data.keys():\n    for key2 in data[key1]:\n        print(\n            json.dumps(data[key1][key2], indent=2)\n        )\n        break\n    break\n\n{\n  \"name_en\": \"Petition for Help\",\n  \"name_jp\": \"\\u6551\\u63f4\\u306e\\u8acb\\u9858\\u66f8\",\n  \"info_en\": \"Summons Stalker to face invading Broken Finger\",\n  \"info_jp\": \"\\u6f70\\u308c\\u6307\\u306b\\u4fb5\\u5165\\u3055\\u308c\\u305f\\u6642\\u3001\\u6f70\\u308c\\u72e9\\u308a\\u3092\\u6551\\u63f4\\u53ec\\u559a\\u3059\\u308b\",\n  \"caption_en\": \"Online multiplayer item. Receipt of a plea for\\r\\nhelp to the maidens of the Finger Reader.\\r\\n\\r\\nSummons a Broken Finger Stalker from another\\r\\nworld to face an invading Broken Finger.\\r\\n\\r\\nMaidens of the Finger Reader speak in hushed\\r\\ntones about the loathsome, traitorous Broken\\r\\nFingers and the dangers of their base invasions.\",\n  \"caption_jp\": \"\\u30aa\\u30f3\\u30e9\\u30a4\\u30f3\\u30d7\\u30ec\\u30a4\\u5c02\\u7528\\u30a2\\u30a4\\u30c6\\u30e0\\r\\n\\u6307\\u8aad\\u307f\\u306e\\u5deb\\u5973\\u305f\\u3061\\u306b\\u8acb\\u9858\\u3057\\u305f\\u8a3c\\r\\n\\r\\n\\u6f70\\u308c\\u6307\\u306b\\u4fb5\\u5165\\u3055\\u308c\\u305f\\u6642\\r\\n\\u4ed6\\u4e16\\u754c\\u304b\\u3089\\u6f70\\u308c\\u72e9\\u308a\\u3092\\u6551\\u63f4\\u53ec\\u559a\\u3059\\u308b\\r\\n\\r\\n\\u6307\\u8aad\\u307f\\u306e\\u5deb\\u5973\\u306f\\u3001\\u58f0\\u3092\\u6f5c\\u3081\\u8a9e\\u308b\\u3060\\u308d\\u3046\\r\\n\\u6f70\\u308c\\u6307\\u306e\\u3001\\u553e\\u68c4\\u3059\\u3079\\u304d\\u88cf\\u5207\\u308a\\u3068\\r\\n\\u5351\\u52a3\\u306a\\u4fb5\\u5165\\u306e\\u5371\\u3046\\u3055\\u3092\\r\\n\"\n}\n\n\n\nQuick and dirty data engineering\nThe keys in the JSON object are categories, and every category contains nested dictionaries that describe a document. This dataset contains many fields that are not relevant to the task, so let us clean the data and prepare it so that we can give the chatbot a reliable knowledge base.\n\ncategories = [c for c in data.keys()]\n\n\ncategory_data = []\nfor c in categories:\n    df = pd.DataFrame(data[c]).T\n    df[\"category\"] = c\n    category_data.append(df)\n\ndf = pd.concat(category_data).reset_index()\ndf.head()\n\n\n\n\n\n\n\n\nindex\nname_en\nname_jp\ninfo_en\ninfo_jp\ncaption_en\ncaption_jp\ncategory\neffect_en\neffect_jp\ndialog_en\ndialog_jp\ntype\nform\nid\n\n\n\n\n0\n100\nPetition for Help\n救援の請願書\nSummons Stalker to face invading Broken Finger\n潰れ指に侵入された時、潰れ狩りを救援召喚する\nOnline multiplayer item. Receipt of a plea for...\nオンラインプレイ専用アイテム\\r\\n指読みの巫女たちに請願した証\\r\\n\\r\\n潰れ指に侵入...\naccessories\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n1\n101\nBroken Finger Stalker Contract\n潰れ狩りの誓約書\nBe summoned to worlds invaded by Broken Fingers\n潰れ指に侵入された世界に救援召喚される\nOnline multiplayer item. Record of contract wi...\nオンラインプレイ専用アイテム\\r\\n指読みの巫女たちと誓約した証\\r\\n\\r\\n他プレイヤー...\naccessories\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n2\n1000\nCrimson Amber Medallion\n緋琥珀のメダリオン\nRaises maximum HP\nＨＰの最大値を上昇させる\nA medallion with crimson amber inlaid.\\r\\nBoos...\n緋色の琥珀が嵌めこまれたメダリオン\\r\\nＨＰの最大値を上昇させる\\r\\n\\r\\n琥珀とは、...\naccessories\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n3\n1001\nCrimson Amber Medallion +1\n緋琥珀のメダリオン＋１\nGreatly raises maximum HP\nＨＰの最大値を大きく上昇させる\nA medallion with crimson amber inlaid.\\r\\nGrea...\n緋色の琥珀が嵌めこまれたメダリオン\\r\\nＨＰの最大値を大きく上昇させる\\r\\n\\r\\n琥珀...\naccessories\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n4\n1002\nCrimson Amber Medallion +2\n緋琥珀のメダリオン＋２\nVastly raises maximum HP\nＨＰの最大値を、とても大きく上昇させる\nA medallion with crimson amber inlaid.\\r\\nVast...\n緋色の琥珀が嵌めこまれたメダリオン\\r\\nＨＰの最大値を、とても大きく上昇させる\\r\\n\\r...\naccessories\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n\n\n\n\n\nWe will focus only on the English texts and keep some other metadata. Moreover, only some of the categories contain documents that might help us give the chatbot the necessary knowledge base:\n\nkeep_columns = [\n    \"id\",\n    \"index\",\n    \"name_en\",\n    \"info_en\",\n    \"caption_en\",\n    \"dialog_en\",\n    \"type\",\n    \"form\",\n    \"category\",\n]\n\ndf = df[keep_columns]\n\n\n\n\n\n\n\nNote\n\n\n\nIn an earlier version of this application, I chose to remove some categories such as those containing system messages and mechanic descriptions unrelated to the lore. However, upon testing the system, I found that some documents in those categories did contain relevant snippets of information.\nTake these nuances into account when developing your prototypes!\n\n\nThe data cleaning process will be as follows:\n\nRemove rows with null values in the name_en column.\nRename this column as the document title.\nConcatenate info_en and caption_en.\nCoalesce caption_en and dialog_en.\n\nAfter this, the resulting data will be nicely transformed into a simpler format containing only the document title, its content and category.\n\ndf = df.dropna(subset=\"name_en\")\ndf[\"title\"] = df[\"name_en\"]\n\ndf[\"info_caption\"] = df[\"info_en\"].fillna(\"\") + \". \" + df[\"caption_en\"].fillna(\"\")\ndf[\"description\"] = df[\"info_caption\"].combine_first(df[\"dialog_en\"])\n\nSome of the text strings in the description column contain extraneous characters and unwanted tokens. Let’s clean those too:\n\nSome of the descriptions contain tagged IDs, e.g. [9000010]\nSome descriptions contain the token (dummyText)\nThere are some duplicated descriptions in the case of upgraded items (e.g “Black Knife Tiche +4” and “Black Knife Tiche +5” have the same description.)\n\n\ndef process_text(df: pd.DataFrame, col_name: str):\n    df.loc[:, col_name] = [re.sub(r'\\[\\d+\\]', '', x) for x in df[col_name]]\n    df.loc[:, col_name] = [re.sub(r'\\(dummyText\\)', '', x) for x in df[col_name]]\n    df = df.drop_duplicates(subset=[col_name])\n\n    return df\n\ndf = process_text(df, \"description\")\n\nFinally, select the relevant columns and save the resulting dataframe. Let’s also run a few sanity checks to spot leftover nulls or duplicates:\n\noutput = df[[\n    \"title\",\n    \"description\",\n    \"category\",\n]]\n\n\noutput.isnull().sum()\n\ntitle          0\ndescription    0\ncategory       0\ndtype: int64\n\n\n\noutput[\"description\"].duplicated().sum()\n\n0\n\n\nNice! The documents are now clean and easy to parse. To make it easier to use the data in other tasks later on, I’ll save it again as a JSON/dictionary. This way, it can be smoothly integrated into different processes, especially when the data processing and chatbot logics are kept separate.\n\ndata = output.to_dict(orient=\"records\")"
  },
  {
    "objectID": "posts/20240402-rag-system-with-langchain/rag-system.html#build-the-rag-system",
    "href": "posts/20240402-rag-system-with-langchain/rag-system.html#build-the-rag-system",
    "title": "Understanding RAG systems with a practical application using locally executed LLMs",
    "section": "Build the RAG system",
    "text": "Build the RAG system\nNow onto the cool(er) stuff!\nLet’s summarize how a RAG system works in three (very) simplified steps:\n\nThe user prompts the system with a question.\nThe question is matched against the knowledge base, which is already transformed through embeddings in the vectorstore.\nThe returned context is fed to the LLM which can now return an informed response based on the given context.\n\nThe first thing we will need to work on is how to store the documents so that the system can find them in the efficiently, and semantically. Using embeddings and vectorstores with LangChain is almost trivial since they are mostly plug-and-play.\n\nRetrieval Setup\nLet’s import the libraries needed for this step, mostly related to LangChain:\n\nfrom langchain.docstore.document import Document\nfrom langchain.embeddings import HuggingFaceEmbeddings\nfrom langchain.vectorstores import DocArrayInMemorySearch\n\nThere is a JSONLoader() class available in LangChain, but I found it easier to just create the individual documents since they are not many and we purposefully prepared it in a tidy format. For each record in the data, a document is created containing the description field, while the title and category are kept as metadata:\n\ndocs = []\nfor record in data:\n    new_document = Document(\n        page_content=record[\"description\"],\n        metadata={\"title\": record[\"title\"], \"category\": record[\"category\"]},\n    )\n    docs.append(new_document)\n\ndocs[42]\n\nDocument(page_content='Boosts dexterity, raises attack power with successive attacks. Part of the golden prosthesis used by Millicent.\\r\\nThe hand is locked into a fist that once raised a sword aloft.\\r\\n\\r\\nBoosts dexterity and raises attack power with successive attacks.\\r\\n\\r\\nThe despair of sweet betrayal transformed Millicent from a mere bud into a magnificent flower. And one day, she will be reborn—as a beautiful scarlet valkyrie.', metadata={'title': \"Millicent's Prosthesis\", 'category': 'accessories'})\n\n\nNext, we need to use pretrained embeddings to transform the documents when loading them into the vectorstore. I’ll just use the HuggingFace embbedings but there are many other options available in LangChain.\n\nembeddings = HuggingFaceEmbeddings()\n\ndb = DocArrayInMemorySearch.from_documents(docs, embeddings)\nretriever = db.as_retriever(search_type=\"mmr\", search_kwargs={\"k\": 5});\n\nOnce the documents are vectorized, they are loaded into the vectorstore which we can use as a retriever for our chatbot chain. Some insight into the parameters chosen here:\n\nsearch_type=\"mmr\": stands for Maximum Marginal Relevance, a search measure that aims to reduce redundancy and increase diversity in search results\nk=5: the number of hits to return, using 5 in this case to avoid retrieving possibly unrelated context or surpassing the input token limit\n\nBehind the scenes, the retriever will perform similarity search and return the results to the chat agent:\n\ndb.similarity_search(\"Black Knife\", search_type=\"mmr\", k=5)\n\n[Document(page_content='Gauntlets used by the Black Blade Assassins. Gauntlets used by the Black Knife Assassins.\\r\\nCrafted with scale armor that makes no sound.\\r\\n\\r\\nThe assassins that carried out the deeds of the Night of the Black Knives were all women, and rumored to be Numen who had close ties with Marika herself.', metadata={'title': 'Black Knife Gauntlets', 'category': 'protector'}),\n Document(page_content='. Dagger once belonging to one of the assassins who murdered Godwyn the Golden on the Night of the Black Knives.\\r\\n\\r\\nA ritual performed on the oddly misshapen blade imbued it with the power of the stolen Rune of Death.', metadata={'title': 'Black Knife', 'category': 'weapon'}),\n Document(page_content='Simple map showing location of black knifeprint\\r\\nExamine using &lt;?keyicon@31?&gt;. A simple map given by Fia.\\r\\n\\r\\nA clue to the whereabouts of a black knifeprint.', metadata={'title': 'Knifeprint Clue', 'category': 'goods'}),\n Document(page_content='. Dagger with a bloodstained blade.\\r\\nAfflicts targets with blood loss.\\r\\n\\r\\nAs blood darkened the dagger through repeated slashing and stabbing, its blade only grew sharper and harder.', metadata={'title': 'Bloodstained Dagger', 'category': 'weapon'}),\n Document(page_content='Armor used by the Black Blade Assassins. Armor used by the Black Knife Assassins.\\r\\nCrafted with scale armor that makes no sound.\\r\\n\\r\\nThe assassins that carried out the deeds of the Night of the Black Knives were all women, and rumored to be Numen who had close ties with Marika herself.', metadata={'title': 'Black Knife Armor (Altered)', 'category': 'protector'})]"
  },
  {
    "objectID": "posts/20240402-rag-system-with-langchain/rag-system.html#llm-setup",
    "href": "posts/20240402-rag-system-with-langchain/rag-system.html#llm-setup",
    "title": "Understanding RAG systems with a practical application using locally executed LLMs",
    "section": "LLM Setup",
    "text": "LLM Setup\nLanguage models are usually tuned so that they excel at specific tasks, such as text summarization or translation. In this case, we would need a question answering model for optimal results, but you can also use general models such as GPT.\nSince we are aiming to use a local LLM, there are some extra steps we need to take:\n\nRun the LLM locally\n\n1. Download the GGUF model file\nGGUF files are already quantized which helps the model speed up inference. For this project, I used WizardLM-13B but you can use smaller models such a the 7B version which will run faster but perform worse.\nAs for the quantization level, Q4_K_M is a good option from my limited experience, because the task doesn’t require a very high degree precision and correctness, so lower quant levels may be acceptable. For other tasks that might require more precision such as coding, higher quants (or none at all) should be used.\n\n\n2. Compile the model\nThe LlamaCpp class allows for the model to be loaded and easily interfaced with other LangChain components:\n\nfrom langchain_community.llms import LlamaCpp\n\nllm = LlamaCpp(\n    model_path=\"../../../sandbox/erdbot/models/WizardLM/wizardlm-7b-v1.0-uncensored.Q4_K_M.gguf\",\n    temperature=0.3,\n    max_tokens=4096,\n    n_ctx=2048,\n);\n\n\n\n\nAlternative: Use the Hugging Face Inference API\nIf you don’t have enough computing resources, you can substitute the LLM component with the ChatHuggingFace class and use the free (and rate-limited) Inference API provided by HF.\nYou can also use any other API keys if you have access to other AI providers such as OpenAI or Azure OpenAI. Just use their the corresponding LangChain interfaces and swap the component in the chain. This guide will only cover the local and Inference API cases.\n\n# Not executed: alternative to the local LLM\n# Note that in this case, the chain expects a ChatModel object and not a text LLM\nfrom langchain_community.llms import HuggingFaceEndpoint\nfrom langchain.chat_models import ChatHuggingFace\n\n\nllm = HuggingFaceEndpoint(\n    repo_id=\"deepset/roberta-base-squad2\",\n    huggingfacehub_api_token=HUGGINGFACEHUB_API_TOKEN,\n    model_kwargs={\"max_length\": -1}, # unlimited response length to avoid truncated answers\n)\n\nchat_model = ChatHuggingFace(llm=llm)\n\nYou can choose from the many models available on the HF Hub, instead of being limited to only quantized models such as in the local case. For this example, I just used roberta-base for QA."
  },
  {
    "objectID": "posts/20240402-rag-system-with-langchain/rag-system.html#design-the-language-chain",
    "href": "posts/20240402-rag-system-with-langchain/rag-system.html#design-the-language-chain",
    "title": "Understanding RAG systems with a practical application using locally executed LLMs",
    "section": "Design the Language Chain",
    "text": "Design the Language Chain\nThere are three main components for a chain: the prompt template, the language model, and (optionally) an output parser.\nLangChain’s API can be somewhat obtuse at times, especially when building chains with multiple arguments, so I’ll try my best to explain what’s going on.\n\nPrompt Design and Engineering\nSome research (i.e. Reynolds & McDonell, 2021) on LLMs shows that a carefully crafted and directed prompt can yield better results. In fact, a whole new subfield of prompt engineering is emerging, contributing to the state of the art with methods such as few shot learning, chain-of-thought and self-reflexion.\nFurthermore, prompt design seems to be a very iterative process. As you will discover when developing your own applications, it often takes a good few tries to create a prompt that consistenly generates the desired outputs. After some time tweaking the template, I came up with the following prompt for our loremaster chatbot:\n\nfrom langchain.prompts import ChatPromptTemplate\n\ntemplate = \"\"\"\nYou are an expert historian studying the lore of an ancient civilization. \nTo answer the user's question, use the following context:\n{context}\n\nOnly include contextual information that not relevant to the user's question in your answer.\nIf you can't infer an answer based on the provided context, explicitly say so. \nDo not invent or hallucinate your responses but try to find likely relationships and connections among the documents.\nBe concise but thorough, and use no more than 5 sentences in your response.\n\nQuestion: {question}\n\"\"\"\nprompt = ChatPromptTemplate.from_template(template)\n\nSome things to note:\n\nThe prompt describes the role the chat agent should take. Other applications might need different agents roles such as “a helpful assistant” or “a teacher grading a math submission”. These should be tweaked based on your use case.\nI tried to find a balance between making likely connections between semantically unrelated documents and avoid hallucinations.\nNotice the f-string formatted variables {context} and {question}, which are a placeholder for the prompt inputs.\n\n\n\nChains with LCEL\nThe next step is to build the actual language chain that will mesh together all the components of the system. In this example, I will be using the LangChain Expression Language (LCEL) that allows the usage of the pipe (|) operator to chain operators and enhance readability.\n\nfrom langchain_core.runnables import RunnableParallel\nfrom operator import itemgetter\n\ncontext = itemgetter(\"question\") | retriever\nanswer = prompt | llm\n\nchain = {\n    \"context\": context,\n    \"question\": itemgetter(\"question\"),\n} | RunnableParallel({\"answer\": answer, \"context\": context})\n\n\ncontext = itemgetter(\"question\") | retriever: the context is the result of sending the question to the vector database (the retriever) and getting document matches back\n\"question\": itemgetter(\"question\"): this simply grabs the question (the user input) from the prompt\nanswer = prompt | llm: the answer is the result of passing the prompt through the LLM\nRunnableParallel({\"answer\": answer, \"context\": itemgetter(\"context\")}): the answer is the result of passing the prompt through the LLM, and the context is the one obtained in the first step\n\nHere, the input to the prompt is expected to be a dictionary with the keys “context” and “question”. The user input is just the question, so we need to get the context using our retriever and passthrough the user input under the “question” key.\nI’m not excessively proficient on LangChain so I’m sure there are better ways to write this chain to enhance readability. If you know of any potential improvements, feel free to let me know!\n\nresponse = chain.invoke({\"question\": \"Who were the Black Knives?\"})\nresponse\n\n\nllama_print_timings:        load time =    1175.93 ms\nllama_print_timings:      sample time =      12.09 ms /    58 runs   (    0.21 ms per token,  4797.35 tokens per second)\nllama_print_timings: prompt eval time =   77701.33 ms /   687 tokens (  113.10 ms per token,     8.84 tokens per second)\nllama_print_timings:        eval time =   11026.84 ms /    57 runs   (  193.45 ms per token,     5.17 tokens per second)\nllama_print_timings:       total time =   88983.37 ms /   744 tokens\n\n\n{'answer': '\\nAnswer: The Black Knives were a group of assassins who were rumored to be Numen who had close ties with Marika herself during the Night of the Black Knives. They were all women and were responsible for carrying out deeds during that eventful night.',\n 'context': [Document(page_content='Gauntlets used by the Black Blade Assassins. Gauntlets used by the Black Knife Assassins.\\r\\nCrafted with scale armor that makes no sound.\\r\\n\\r\\nThe assassins that carried out the deeds of the Night of the Black Knives were all women, and rumored to be Numen who had close ties with Marika herself.', metadata={'title': 'Black Knife Gauntlets', 'category': 'protector'}),\n  Document(page_content=\". Unique curved sword, notched like shark's teeth.\\r\\nWeapon carried by corpse pillagers who prowl the sites of old battles.\\r\\n\\r\\nThe blade is tacky with blood and covered in hefty nicks, making it totally uneven. Life can be sinister indeed.\", metadata={'title': \"Scavenger's Curved Sword\", 'category': 'weapon'}),\n  Document(page_content='Throw fanned-out knives at enemies to inflict damage. A set of five throwing knives bundled together.\\r\\nA concealed weapon cherished by the raptor assassins.\\r\\n\\r\\nThe thin knives fan out when thrown, dealing damage to the target.\\r\\n\\r\\nEach knife deals paltry damage, but the wide range makes it suitable for constraining enemies.', metadata={'title': 'Fan Daggers', 'category': 'goods'}),\n  Document(page_content=\"Mark of the Night of the Black Knives ritual. On the Night of the Black Knives, someone stole a fragment of Death from Maliketh, the Black Blade, and imbued its power into the assassins' daggers.\\r\\n\\r\\nThis mark is evidence of the ritual, and hides the truth of the conspiracy.\", metadata={'title': 'Black Knifeprint', 'category': 'goods'}),\n  Document(page_content='. Curved greatswords of black steel wielded by General Radahn.\\r\\nA pair of weapons decorated with a lion mane motif.\\r\\n\\r\\nRadahn earned considerable renown as the Starscourge in his youth, and it is said that it was during this time he engraved the gravity crest upon these blades.', metadata={'title': 'Starscourge Greatsword', 'category': 'weapon'})]}\n\n\nLet’s take a look at our output here:\n\nTimings: these are not very interesting right now, but they can be useful when attempting to optimize your application. For example, this interaction had a total response time of over 20 seconds, which is probably not great for a production application.\nOutput: in this chain, I chose to output both the answer and the context so we can analyse the information that influenced the LLM’s response.\n\nThis is the answer an end user would get:\n\nprint(\n    response[\"answer\"].replace(\". \", \".\\n\")\n)\n\nAnswer: The Black Knives were a group of assassins who carried out the Night of the Black Knives, a secretive event that occurred in the past.\nThey were rumored to be Numen who had close ties with Marika herself.\nThe assassins were all women and were known to be skilled in combat and stealth.\nThey were also known to be equipped with unique weapons such as the Black Knife Gauntlets and Scavenger's Curved Sword.\nThe Night of the Black Knives was a conspiracy that involved stealing a fragment of Death from Maliketh, the Black Blade, and imbuing its power into the assassins' daggers.\nThe Mark of the Night of the Black Knives ritual was also performed on this night, and it is believed that this ritual hides the truth of the conspiracy.\n\n\nThe results are somewhat sensible, but take a closer look at the following statement:\n\nThey were also known to be equipped with unique weapons such as the Black Knife Gauntlets and Scavenger’s Curved Sword.\n\nThis mentions the Scavenger’s Curved Sword. However, if you read the document from the context that originated this fragment, you will see that it has no relation to the Black Knives at all. In these cases, consider the following methods:\n\nlower the k value used in similarity search to reduce the number of relevant results\nchange the search_type argument depending on your needs, since in the example there are many other documents containing the exact substring “Black Knives” that do not appear in the context due to using maximum marginal relevance, which penalizes redundancy\n\nThis shows that domain knowledge may be useful when evaluating and debugging these systems. So, while we’re on the topic:"
  },
  {
    "objectID": "posts/20240402-rag-system-with-langchain/rag-system.html#optimization-and-advanced-features",
    "href": "posts/20240402-rag-system-with-langchain/rag-system.html#optimization-and-advanced-features",
    "title": "Understanding RAG systems with a practical application using locally executed LLMs",
    "section": "Optimization and Advanced Features",
    "text": "Optimization and Advanced Features\nThis is the very basics of a RAG architecture. There’s not much to do with it while it’s confined to a notebook, so here are a few ways you can take it a step further:\n\nPrompt optimization: aside from fine-tuning your prompt manually, you can try letting the LLM write it for you! Automatically generated prompts1 have proven to perform slighly better than some hand-tuned prompts.\nEvaluation: use the RAG triad of measures to assess the performance of your application. One simple and straight-forward way to do it is to ask the LLM itself (or another one prepared for an evaluation task) to grade the generated response based on the provided context.\nDeployment: so far, Streamlit is the easiest way I’ve found to interact with the system in a chat-like enviroment. Check out this short guide.\nScalability: in a laptop or any other mid-tier machine, the response times of the LLM will probably be quite high. Consider using smaller models, or cloud instances for hosting your application in any of the commercial hyperscalers.\n\n1 Battle, R., & Gollapudi, T. (2024). The Unreasonable Effectiveness of Eccentric Automatic PromptsNew and exciting stuff is coming to light every few days in this field, so keep an eye open and you’re sure to find new improvements for your application."
  },
  {
    "objectID": "posts/20240402-rag-system-with-langchain/rag-system.html#conclusion",
    "href": "posts/20240402-rag-system-with-langchain/rag-system.html#conclusion",
    "title": "Understanding RAG systems with a practical application using locally executed LLMs",
    "section": "Conclusion",
    "text": "Conclusion\nThat’s it! Your very own RAG application now lives and runs on your computer, as long as it doesn’t spontaneously combust during the inference process.\nFeel free to test it with your own data as well, since LangChain offers different loader classes to read from webpages, PDF documents and the like. The advent of somewhat easily accesible LLMs opens up a myriad possibilities for new projects and ideas. It is a constantly evolving landscape and you can get creative with stuff like chatbots, research assistants, or content generators. The skills from this guide are fairly basic but building on them can take your NLP game to the next level.\nNow go forth and deploy something cool with RAG tech!"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Me",
    "section": "",
    "text": "I’m a machine learning engineer who enjoys writing code to build data-driven solutions, with experience across analytics, visualization, and production-grade ML systems."
  },
  {
    "objectID": "about.html#work",
    "href": "about.html#work",
    "title": "About Me",
    "section": " Work",
    "text": "Work\n\nSenior Consultant, Data Science and ML @ SDG Group (May 2025 - present)\nConsultant, Data Science and ML @ SDG Group (Mar 2023 - May 2025)\nAI/ML Engineer @ Bravent (Jan 2022 - Feb 2023)\nData Scientist @ Tinamica (Jan 2020 - Jan 2022)"
  },
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "About Me",
    "section": " Education",
    "text": "Education\n\nMSc Big Data Analytics for Economics and Business @ Universitat de les Illes Balears\nBSc Economics @ Universitat de les Illes Balears"
  },
  {
    "objectID": "about.html#interests",
    "href": "about.html#interests",
    "title": "About Me",
    "section": " Interests",
    "text": "Interests\n\nStatistics and Data\nTech and Engineering\nSkateboarding\nReading, writing and coding\nMusic, many kinds\nGaming"
  },
  {
    "objectID": "posts/20200104-aseprite-from-source/index.html",
    "href": "posts/20200104-aseprite-from-source/index.html",
    "title": "How to build Aseprite from source in Ubuntu",
    "section": "",
    "text": "Source: aseprite.org"
  },
  {
    "objectID": "posts/20200104-aseprite-from-source/index.html#building-aseprite-from-github",
    "href": "posts/20200104-aseprite-from-source/index.html#building-aseprite-from-github",
    "title": "How to build Aseprite from source in Ubuntu",
    "section": "Building Aseprite from Github",
    "text": "Building Aseprite from Github\nAseprite is the best pixel art tool I have come across, and has a very affordable price if you just want to test the waters of pixel art. However, should you want to help tackle some of the issues on the project’s repository, you need to compile the program to inspect your changes.\nI went through this process on my Windows and Linux machines, and I must say that the experience was wildly different, even if the process is very similar. I am currently using Ubuntu 18.04 LTS."
  },
  {
    "objectID": "posts/20200104-aseprite-from-source/index.html#get-started",
    "href": "posts/20200104-aseprite-from-source/index.html#get-started",
    "title": "How to build Aseprite from source in Ubuntu",
    "section": "Get started",
    "text": "Get started\nBefore beginning, please note that the full install instructions can be found in the project’s repo.\nFirst, you should clone the repository in a directory of your choice:\ngit clone --recursive https://github.com/aseprite/aseprite.git\nThis will create an aseprite directory. You can later update this clone with new releases by running:\ncd aseprite\ngit pull\ngit submodule update --init --recursive"
  },
  {
    "objectID": "posts/20200104-aseprite-from-source/index.html#aseprite-dependencies",
    "href": "posts/20200104-aseprite-from-source/index.html#aseprite-dependencies",
    "title": "How to build Aseprite from source in Ubuntu",
    "section": "Aseprite dependencies",
    "text": "Aseprite dependencies\nThere are three main dependencies for Aseprite:\n\nlatest CMake\nthe Ninja build system\na compiled version of Skia\n\nNote that for Windows, you need Windows 10 and Visual Studio 2019, as well as a [C++ SDK] for desktop development. The latter is an item to check in your Visual Studio installer, so make sure you have selected it.\nAlso, installing CMake on Windows was HORRIBLE and took quite a long time. In Ubuntu, however, I was able to install all dependencies with a single command:\nsudo apt-get install -y g++ cmake ninja-build libx11-dev libxcursor-dev libgl1-mesa-dev libfontconfig1-dev\nBuilding Skia is also quite trivial on Linux. I was able to run all of the following commands without any errors or missing dependencies, unlike in Windows:\nmkdir $HOME/deps\ncd $HOME/deps\ngit clone https://chromium.googlesource.com/chromium/tools/depot_tools.git\ngit clone -b aseprite-m71 https://github.com/aseprite/skia.git\nexport PATH=\"${PWD}/depot_tools:${PATH}\"\ncd skia\npython tools/git-sync-deps\ngn gen out/Release --args=\"is_debug=false is_official_build=true skia_use_system_expat=false skia_use_system_icu=false skia_use_system_libjpeg_turbo=false skia_use_system_libpng=false skia_use_system_libwebp=false skia_use_system_zlib=false\"\nninja -C out/Release skia\nThese commands can also be found in the Aseprite documentation, so head there if you get stuck."
  },
  {
    "objectID": "posts/20200104-aseprite-from-source/index.html#compiling-the-code",
    "href": "posts/20200104-aseprite-from-source/index.html#compiling-the-code",
    "title": "How to build Aseprite from source in Ubuntu",
    "section": "Compiling the code",
    "text": "Compiling the code\nAfter getting all the dependecies, create a new folder build inside your aseprite directory, and cd into it. Afterwards, run cmake:\ncd ~/aseprite\nmkdir build\ncd build\n\nIn this last command, cmake needs a different setup based on your OS. I will just outline the Linux process since I still get nightmares trying to install Clang, Google depot tools, CMake and Visual Studio in Windows. Feel free to look up the other ones in the installation guide.\n\nRun this command on your Linux machine to build the source:\ncmake \\\n  -DCMAKE_BUILD_TYPE=RelWithDebInfo \\\n  -DLAF_OS_BACKEND=skia \\\n  -DSKIA_DIR=$HOME/deps/skia \\\n  -DSKIA_OUT_DIR=$HOME/deps/skia/out/Release \\\n  -G Ninja \\\n  ..\nWhen your terminal finishes the previous task, just run Ninja inside the build directory and you will have your executable in aseprite/build/bin/aseprite:\nninja aseprite"
  },
  {
    "objectID": "posts/20200104-aseprite-from-source/index.html#start-pixeling",
    "href": "posts/20200104-aseprite-from-source/index.html#start-pixeling",
    "title": "How to build Aseprite from source in Ubuntu",
    "section": "Start pixeling!",
    "text": "Start pixeling!\nThat should be it! Run your Aseprite executable and you should be greeted by the clean interface of the software. Remember to actually purchase Aseprite if you are going to use it commercially or professionally."
  },
  {
    "objectID": "posts/20200101-data-visualization-principles/index.html",
    "href": "posts/20200101-data-visualization-principles/index.html",
    "title": "Data visualization principles",
    "section": "",
    "text": "Note\n\n\n\nUpdated on 2020-1-23 with figures and examples, like a good post about data visualization.\n\n\nWhen designing a visualization, there are usually four main topics that the author would want to highlight:\n\nComparison\nThese are used to compare the magnitude of values to each other and to easily identify the lowest or highest values in the data.\nIf you want to compare values over time, line or bar charts are often the best option and their choice depends on the number of periods you would like to analyze. However, for comparisons among items, bar or column charts are preferred. Line charts provide a sense of continuity that might not be meaningful for categories.\nFaceted charts are also a good option when dealing with many categories or dimensions. R’s ggplot2 package is specially useful for this.\n\n\n\nExample of a faceted plot\n\n\nFor example, the above plot shows the density of each numerical variable in a dataset, using subplots or facets to create a new dimension of visualization.\nYou can use pie charts for comparison as well, although the length of a column or bar is usually better at displaying differences in your data than the angle of a pie chart.\nFor example, examine the following figures:\n\n\n\nPie chart against bar chart\n\n\nFor values differing very little, such as 10% to 11%, the difference is more noticeable when plotted in a bar chart with proper scaling. Labels normally alleviate this problem, but they also tend to increase clutter in the chart.\nHowever, I am not completely against pie charts, as I think they can quick convey meaning when plotting attributes of low cardinality or dichotomic relations. Personally, I think 2 (or at most 3) different values should be shown on a pie chart so that it does not become confusing.\n\n\nComposition\nComposition charts are used to see how a part of your data compares to the whole, and can show relative and absolute values. They can be used to accurately represent both static and time-series data.\nFor static data, a pie chart can do the job. However, there also other options that can tell the same story, such as a stacked bar chart, a waterfall chart or a tree map. For time-based data, the number of periods is again a decisive factor when choosing your chart. You can visualize composition over many periods with area charts, which are very similar to line charts, and stacked bar or column charts when you have a reduced amount of periods.\n\n\n\nExample of composition chart\n\n\nIt is important to consider that time-based visualization should be ordered chronologically as a general rule, and not by highest value, for example. Otherwise, those consuming the visualization might get the wrong conclusion when charts are not ordered according to expectations or convention.\n\n\nDistribution\nWhen studying how quantitative values are located along an axis, distribution charts are the way to go. By looking at the shape of the data, the user can identify features such as value range, central tendency and outliers.\nOne of the previous figures is actually an example of a distribution plot. Here’s another one:\n\n\n\nDensity plot to show value distribution\n\n\nSee how the relative values of the population are distributed along the different age categories. Being a continuous range of integers, we use lines to represent the distribution. If instead, it were categories (i.e. age 18 - 35) we would use columns.\nWith this charts, the interest is in the full picture of the data and this can lead to having many data points (note: not categories!). In these cases, it’s often better to use a line histogram, while column histograms are great for few data points. In any case, when analyzing distribution for two variables at once, a scatter plot allows to compare between the full picture of two dimensions.\n\n\nRelationship\nRelationship charts have a constrained set of options as normally scatter plots are the only adequate way of presenting the data. They are used to find correlations, outliers, and clusters in your data.\nWhile the human eye can only appreciate three dimensions together, you can visualize additional variables by mapping them to the size, color or shape of your data points.\n\n\n\nExample of a scatter plot\n\n\nIn this example, the chart can be used to study 3 variables at the same time: number of marketing calls, their duration, and whether their outcome is a sale or not. To aid the human eye, the third dimension is mapped to a color based on the value of their factors. Thanks to this, we can see that calls that do not result in a sale are concentrated along the lower bounds of the Y axis, meaning more calls increases the likelihood of conversion.\nSee the image below for reference:\n\n\n\nVisualization Best Principles\n\n\n\n\n\n\n\n\nTip\n\n\n\nAs a bonus tool, if you run into issues trying to find the best set of colors for your report or chart, make sure to check out Coolors. This is a web app that provides color scales, palettes and blends based on the color of your choice.\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "posts/index.html",
    "href": "posts/index.html",
    "title": "Writing",
    "section": "",
    "text": "Order By\n      Default\n      \n        Title\n      \n      \n        Date - Oldest\n      \n      \n        Date - Newest\n      \n    \n  \n\n\n\n\n\n\n\n\n\n\nWhy I Migrated My Blog from Pelican to Quarto\n\n\nA simpler path to technical writing\n\n\n\nPython\n\nWriting\n\nData Science\n\n\n\n\n\n\nJul 23, 2025\n\n5 min\n\n\n\n\n\n\n\n\n\n\n\nUnderstanding RAG systems with a practical application using locally executed LLMs\n\n\nExplore the lore of Elden Ring through natural language\n\n\n\nAI\n\nCode\n\n\n\n\n\n\nApr 2, 2024\n\n13 min\n\n\n\n\n\n\n\n\n\n\n\nA comment system for static websites using Github Discussions\n\n\nEasily add comments to your static website with giscus and GitHub Discussions!\n\n\n\nWeb Dev\n\n\n\n\n\n\nApr 23, 2022\n\n3 min\n\n\n\n\n\n\n\n\n\n\n\nA Pint of API\n\n\nUnderstand the fundamentals of APIs with this adaptation of a short talk I gave fully in-editor with Visual Studio Code.\n\n\n\nWeb Dev\n\n\n\n\n\n\nJan 18, 2020\n\n5 min\n\n\n\n\n\n\n\n\n\n\n\nHow to build Aseprite from source in Ubuntu\n\n\nAseprite is an amazing pixel art software and its source code is open. This is how you can build and test it, or help with its development.\n\n\n\nGame Dev\n\n\n\n\n\n\nJan 4, 2020\n\n3 min\n\n\n\n\n\n\n\n\n\n\n\nData visualization principles\n\n\nConveying meaning through data visualization is not an easy task. Here is how you might get started.\n\n\n\nData Visualization\n\n\n\n\n\n\nJan 1, 2020\n\n5 min\n\n\n\n\n\n\n\n\n\n\n\nData Analysis eXpressions query language\n\n\nA brief overview of the DAX language for data analysis.\n\n\n\nData Engineering\n\nPowerBI\n\n\n\n\n\n\nDec 27, 2019\n\n10 min\n\n\n\n\n\n\n\n\n\n\n\nHow I got started with Linux\n\n\nAn overview of my Linux development setup.\n\n\n\nLinux\n\nCode\n\n\n\n\n\n\nDec 8, 2019\n\n4 min\n\n\n\n\n\n\n\n\n\n\n\nAn introduction to Power BI for data scientists\n\n\nPower BI is a great free tool that can be used to quickly prototype visualizations for exploratory data analysis.\n\n\n\nData Visualization\n\nPowerBI\n\n\n\n\n\n\nDec 6, 2019\n\n8 min\n\n\n\n\n\n\n\n\n\n\n\nMaster of Visual Studio Code\n\n\nMaster Visual Studio Code and its features with these tips\n\n\n\nCode\n\n\n\n\n\n\nNov 26, 2019\n\n22 min\n\n\n\n\nNo matching items\n Back to top"
  },
  {
    "objectID": "posts/20191208-linux-dev-env/index.html",
    "href": "posts/20191208-linux-dev-env/index.html",
    "title": "How I got started with Linux",
    "section": "",
    "text": "I recently got the itch of purchasing a new laptop for development and learning, so I took the chance on Black Friday and got myself a nice HP Stream 14. I am a long-time Windows user so I thought it would be a great opportunity to get started with Linux and set up a developer workstation while documenting the process."
  },
  {
    "objectID": "posts/20191208-linux-dev-env/index.html#coding-tools",
    "href": "posts/20191208-linux-dev-env/index.html#coding-tools",
    "title": "How I got started with Linux",
    "section": "Coding tools",
    "text": "Coding tools\nThat’s my default editor and developer tool. I love the extensible and customizable design, and the many productivity hacks you can take advantage of.\nTilix is an awesome terminal emulator with integrated multiplexing (like tmux) and can speed up your workflow significantly if used properly. I also like Hyper as it is very simple and lightweight.\nSince I started dabbling in game development, Godot has been the most approachable engine I have encountered. I tried PhaserJS briefly but my JavaScript is not that polished and I find an engine to be more more hollistic in developmnet as opposed to a framework like Phaser."
  },
  {
    "objectID": "posts/20191208-linux-dev-env/index.html#programming-languages",
    "href": "posts/20191208-linux-dev-env/index.html#programming-languages",
    "title": "How I got started with Linux",
    "section": "Programming languages",
    "text": "Programming languages\nEven though Python comes installed in Linux distros by default, its package manager pip wasn’t available on my system. A swift sudo apt-get install pip3 took that problem away. Take note of the pip3 part, which installs the module for Python 3.\nI also installed Julia 1.3 to run some benchmarks against Python and try my hand at some development with this growing language.\nI added some other stuff, languages like V and Lua to have more options to play with. As you can see, I have installed many different languages so I can mix and match my learning and find the niche I am most comfortable in. Many advocate against a “jack of all trades” approach, but I find breadth of knowledge more useful than depth in general situations. You can always specialise if necessary.\nI am thinking of trying Go, but I will leave that for later because I don’t really want to bite off more than I can chew. Which I am already doing, anyway."
  },
  {
    "objectID": "posts/20191208-linux-dev-env/index.html#closing-thoughts",
    "href": "posts/20191208-linux-dev-env/index.html#closing-thoughts",
    "title": "How I got started with Linux",
    "section": "Closing thoughts",
    "text": "Closing thoughts\nI have been using this setup for a few months now and I am very satisfied with my decision. My Linux desktop is fast and sleek with a good-looking interface, and installing stuff is as easy as writing one shell command. I can definitely see why many developers use Linux, and I will probably make a full switch from Windows when possible."
  },
  {
    "objectID": "posts/20191227-learning-dax/index.html",
    "href": "posts/20191227-learning-dax/index.html",
    "title": "Data Analysis eXpressions query language",
    "section": "",
    "text": "DAX is a functional language, that is, execution flows from function calls read from inner to outer context in nested calls.\nHere is a short description of DAX syntax conventions:\n\nUse [Measure] and ‘Table’[Column] and never otherwise. Quotes can be omitted for single word table names.\nUse spaces before opening and closing parentheses, and before any operand and operator.\nSpace before an in-line argument, and indented line feed for multi-line function calls.\n\n\n\n\n\n\nSUM, AVERAGE, MIN, MAX…\n\nSUM ( Sales[Price] ) works!\nSUM ( Sales[Price] + Sales[Quantity] ) does not!\n\n\nExpressions within aggregators need to be calculated with an iterator. These are versions of the function with an X in their name, i.e. SUMX. These functions iterate over a table and evaluate the expression for every row, which means the concept of row context comes to life here. These functions always receive two inputs: the table to iterate and expression to evaluate.\nNote that SUM is just syntax sugar. SUM ( Sales[Price] ) is translated in-engine to SUMX ( Sales, Sales[Price] )\n\n\n\n\nCOUNT /COUNTA: anything but blanks\nCOUNTBLANK: blanks and empty strings\nCOUNTROWS: rows in a table\nDISTINCTCOUNT: syntax sugar for COUNTROWS ( DISTINCT ( 'Table'[Column] ) )\n\n\n\n\n\nRELATED / RELATEDTABLE: both follow relationships in the data model and return the value of a column or all the rows in relationship with the current one.\n\nAll relationships must follow the same direction.\n\n\n\n\n\nThese return full tables which are often used as the input for further calculations.\nYou can mix table functions, for example: FILTER ( ALL ( 'Sales' ); Product[Name] = \"Laptop\" ) puts a filter over the entire table, ignoring any other filter context.\n\nFILTER, ALL, VALUES, DISTINCT, ADDCOLUMNS, SUMMARIZE…\nDISTINCT returns the unique values of a column that are visible in the current context.\nVALUES is the same, respects the filter context and returns the additional blank row if it’s visible within the context. This row is created to guarantee referential integrity when values in the fact table can’t be found in the dimension table.\nALL returns all the rows of a table while ignoring every filter. It’s useful to calculate grand totals and relative percentages.\nALLNOBLANKROW is the same, but excludes the blank row.\nALLSELECTED returns the elements of a table as visible outside the current visualization. That is, it ignores all filters from the current visual but respects all others.\n\nVery dangerous function as it is difficult to debug.\n\nADDCOLUMNS adds one or more columns to a table expression, keeping all existing columns. It’s an iterator, so you can access columns of the table and perform expressions and calculations.\n\n\n\nI often use ADDCOLUMNS to generate a Date table, For example:\n\nADDCOLUMNS (\n    ALL ( 'Sales'[sk_Date] ); # the table that will be iterated\n    \"Month\"; # the name of the first column to add\n    MONTH ( 'Sales'[sk_Date] ); # the definition of the first column to add\n    \"Year\"; \n    YEAR ( 'Sales'[sk_Date] ); \n    ...\n)\n\nThis is a very flexible way of generating your own date dimension according to your specific needs.\nYou can also generate a dimension table by scanning the columns of your different fact tables and appending them. For example:\n\nDISTINCT (\n    UNION (\n        ALL ( LaptopOrders[OrderID] );\n        ALL ( MobileOrders[OrderID] );\n    )\n)\n\n\n\n\nVerifies whether a value exists in a list of possible values, i.e. Company[Code] IN {\"Microsoft\", \"Apple\", \"Dell\"}. Note that the curly brackets notation generates a temporary table behind the scenes to perform the filtering.\n\n\n\n\nDIVIDE ( ): handles safe division by zero and allows for custom alternate values. No more IF ( ISERROR (...) )!\n\nAlso, better performance.\n\nSELECTEDVALUE ( ): provides a shortcut to access the filtered value of a dimension to display in a chart or card.\n\nFor example, you can create a custom measure that concatenates text with the selected value and display dynamic title in a chart.\nPrior to this function, the same objective could be accomplished through something along the lines of IF ( HASONEVALUE ( ... ) ), which is much longer and error-prone.\n\n\n\n\n\nCALCULATE is the main function for DAX expressions, as it is used in many different scenarios to add, change or remove filters. Its syntax is as follows:\nCALCULATE (\n    Expression;\n    Filter1;\n    ...;\n    FilterN \n)\nCALCULATE calls with multiple filters can have very different behaviors:\nCALCULATE (\n    SUM ( Orders[ItemId] );\n    Company[Name] IN {\"Microsoft\", \"Apple\"};\n    Date[Year] = 2019\n)\nThis will result in the amount of booking items for the filtered companies, in 2019. However, the following expression has a very different result:\nCALCULATE (\n    CALCULATE (\n        SUM ( Orders[ItemId] );\n        Company[Code] IN {\"Microsoft\", \"Apple\"};\n    );\n    Company[Code] IN {\"Microsoft\", \"Dell\"}\n)\nIn this case, the use of nested CALCULATE statements does not intersect the filter. The result of the expression is the amount of items only for Microsoft, since the inner filter overwrites the outer.\nYou can override this behavior to intersect filter by using the KEEPFILTERS function:\nCALCULATE (\n    CALCULATE (\n        SUM ( Orders[ItemId] );\n        KEEPFILTERS ( Company[Code] IN {\"Microsoft\", \"Apple\"} );\n    );\n    Company[Code] IN {\"Microsoft\", \"Dell\"}\n)\nIn this context, only Microsoft remains visible at the end of the evaluation.\n\n\n\nNewer feature in DAX. Allows for reduced repetition of expressions and other nifty tricks, such as accessing outer row context. You can store strings, numbers, and even tables in a variable.\nVariables can be used to store the previous row context for later access:\nProduct[Rank] =\n\nVAR _CurrentPrice = Product[Price]\n\nRETURN\n\nCOUNTROWS (\n    FILTER(\n        Product;\n        Product[Price] &gt; _CurrentPrice\n    )\n) + 1\nIn this case, you save the value of the previous price during the outer context evaluation, and use it to check whether the price in the inner context is greater."
  },
  {
    "objectID": "posts/20191227-learning-dax/index.html#data-analysis-expressions-dax",
    "href": "posts/20191227-learning-dax/index.html#data-analysis-expressions-dax",
    "title": "Data Analysis eXpressions query language",
    "section": "",
    "text": "DAX is a functional language, that is, execution flows from function calls read from inner to outer context in nested calls.\nHere is a short description of DAX syntax conventions:\n\nUse [Measure] and ‘Table’[Column] and never otherwise. Quotes can be omitted for single word table names.\nUse spaces before opening and closing parentheses, and before any operand and operator.\nSpace before an in-line argument, and indented line feed for multi-line function calls.\n\n\n\n\n\n\nSUM, AVERAGE, MIN, MAX…\n\nSUM ( Sales[Price] ) works!\nSUM ( Sales[Price] + Sales[Quantity] ) does not!\n\n\nExpressions within aggregators need to be calculated with an iterator. These are versions of the function with an X in their name, i.e. SUMX. These functions iterate over a table and evaluate the expression for every row, which means the concept of row context comes to life here. These functions always receive two inputs: the table to iterate and expression to evaluate.\nNote that SUM is just syntax sugar. SUM ( Sales[Price] ) is translated in-engine to SUMX ( Sales, Sales[Price] )\n\n\n\n\nCOUNT /COUNTA: anything but blanks\nCOUNTBLANK: blanks and empty strings\nCOUNTROWS: rows in a table\nDISTINCTCOUNT: syntax sugar for COUNTROWS ( DISTINCT ( 'Table'[Column] ) )\n\n\n\n\n\nRELATED / RELATEDTABLE: both follow relationships in the data model and return the value of a column or all the rows in relationship with the current one.\n\nAll relationships must follow the same direction.\n\n\n\n\n\nThese return full tables which are often used as the input for further calculations.\nYou can mix table functions, for example: FILTER ( ALL ( 'Sales' ); Product[Name] = \"Laptop\" ) puts a filter over the entire table, ignoring any other filter context.\n\nFILTER, ALL, VALUES, DISTINCT, ADDCOLUMNS, SUMMARIZE…\nDISTINCT returns the unique values of a column that are visible in the current context.\nVALUES is the same, respects the filter context and returns the additional blank row if it’s visible within the context. This row is created to guarantee referential integrity when values in the fact table can’t be found in the dimension table.\nALL returns all the rows of a table while ignoring every filter. It’s useful to calculate grand totals and relative percentages.\nALLNOBLANKROW is the same, but excludes the blank row.\nALLSELECTED returns the elements of a table as visible outside the current visualization. That is, it ignores all filters from the current visual but respects all others.\n\nVery dangerous function as it is difficult to debug.\n\nADDCOLUMNS adds one or more columns to a table expression, keeping all existing columns. It’s an iterator, so you can access columns of the table and perform expressions and calculations.\n\n\n\nI often use ADDCOLUMNS to generate a Date table, For example:\n\nADDCOLUMNS (\n    ALL ( 'Sales'[sk_Date] ); # the table that will be iterated\n    \"Month\"; # the name of the first column to add\n    MONTH ( 'Sales'[sk_Date] ); # the definition of the first column to add\n    \"Year\"; \n    YEAR ( 'Sales'[sk_Date] ); \n    ...\n)\n\nThis is a very flexible way of generating your own date dimension according to your specific needs.\nYou can also generate a dimension table by scanning the columns of your different fact tables and appending them. For example:\n\nDISTINCT (\n    UNION (\n        ALL ( LaptopOrders[OrderID] );\n        ALL ( MobileOrders[OrderID] );\n    )\n)\n\n\n\n\nVerifies whether a value exists in a list of possible values, i.e. Company[Code] IN {\"Microsoft\", \"Apple\", \"Dell\"}. Note that the curly brackets notation generates a temporary table behind the scenes to perform the filtering.\n\n\n\n\nDIVIDE ( ): handles safe division by zero and allows for custom alternate values. No more IF ( ISERROR (...) )!\n\nAlso, better performance.\n\nSELECTEDVALUE ( ): provides a shortcut to access the filtered value of a dimension to display in a chart or card.\n\nFor example, you can create a custom measure that concatenates text with the selected value and display dynamic title in a chart.\nPrior to this function, the same objective could be accomplished through something along the lines of IF ( HASONEVALUE ( ... ) ), which is much longer and error-prone.\n\n\n\n\n\nCALCULATE is the main function for DAX expressions, as it is used in many different scenarios to add, change or remove filters. Its syntax is as follows:\nCALCULATE (\n    Expression;\n    Filter1;\n    ...;\n    FilterN \n)\nCALCULATE calls with multiple filters can have very different behaviors:\nCALCULATE (\n    SUM ( Orders[ItemId] );\n    Company[Name] IN {\"Microsoft\", \"Apple\"};\n    Date[Year] = 2019\n)\nThis will result in the amount of booking items for the filtered companies, in 2019. However, the following expression has a very different result:\nCALCULATE (\n    CALCULATE (\n        SUM ( Orders[ItemId] );\n        Company[Code] IN {\"Microsoft\", \"Apple\"};\n    );\n    Company[Code] IN {\"Microsoft\", \"Dell\"}\n)\nIn this case, the use of nested CALCULATE statements does not intersect the filter. The result of the expression is the amount of items only for Microsoft, since the inner filter overwrites the outer.\nYou can override this behavior to intersect filter by using the KEEPFILTERS function:\nCALCULATE (\n    CALCULATE (\n        SUM ( Orders[ItemId] );\n        KEEPFILTERS ( Company[Code] IN {\"Microsoft\", \"Apple\"} );\n    );\n    Company[Code] IN {\"Microsoft\", \"Dell\"}\n)\nIn this context, only Microsoft remains visible at the end of the evaluation.\n\n\n\nNewer feature in DAX. Allows for reduced repetition of expressions and other nifty tricks, such as accessing outer row context. You can store strings, numbers, and even tables in a variable.\nVariables can be used to store the previous row context for later access:\nProduct[Rank] =\n\nVAR _CurrentPrice = Product[Price]\n\nRETURN\n\nCOUNTROWS (\n    FILTER(\n        Product;\n        Product[Price] &gt; _CurrentPrice\n    )\n) + 1\nIn this case, you save the value of the previous price during the outer context evaluation, and use it to check whether the price in the inner context is greater."
  },
  {
    "objectID": "posts/20191227-learning-dax/index.html#evaluation-contexts",
    "href": "posts/20191227-learning-dax/index.html#evaluation-contexts",
    "title": "Data Analysis eXpressions query language",
    "section": "Evaluation contexts",
    "text": "Evaluation contexts\nThe filter and row contexts are one of the most important concepts in DAX.\nThe filter context is defined by:\n\nrow selection\n\ncolumn selection\n\nreport filters\n\nslicer selection\n\nThe row context is the concept of “current row” and you have a row context whenever you iterate a table, either explicitly (using an iterator) or implicitly (in a calculated column).\n\nQuick tip!\nStarting from a row context, you can use related to access columns in other tables:\n\nSalePrice =\n\nSUMX (\n    Orders;\n    Orders[Cost] * RELATED ( Product[Markup] )\n)\n\nYou need RELATED because the row context is iterating the sales table. The Markup columns is in another table, but the row context allows for the relationship to be propagated.\n\nAverageDiscountedIncomeByProduct =\n\nAVERAGEX (\n    Products;\n    SUMX (\n        RELATEDTABLE ( Orders );\n        Orders[SalePrice] * Products[Discount]\n    )\n)\nThe inner expression can safely reference two columns coming from different tables because there are two row contexts: the first one introduced by AVERAGEX and the second one by SUMX. Also note that RELATEDTABLE uses the row context to determine the set of rows to return, that is, the Orders of the current product which is the one being currently iterated by AVERAGEX.\n\nContext transition\nThe context transition in DAX is the transformation of row contexts into an equivalent filter context performed by CALCULATE functions. When CALCULATE is executed within a row context, it transforms the context into the equivalent filter context and applies it to the data model before computing its expression. This is an expensive operation, so it’s better to avoid it when iterating large tables.\nThis concept is easier to understand through examples:\nProduct[SumOfUnitsSold] = SUM ( Orders[UnitsSold] )\nProduct[SumOfUnitsSoldCalculate] = CALCULATE ( SUM ( Orders[UnitsSold] ) )\nThe first calculated column returns the grand total of Orders[UnitsSold], because no filter context is active, whereas the one with CALCULATE returns the sum of Orders[UnitsSold] for the current product only, because the filter context containing the current product is automatically propagated to sales due to the relationship between the two tables.\nIt is important to note that context transition happens before further filters in CALCULATE. Thus, filters in CALCULATE might override filters produced by context transition as seen on previous sections."
  },
  {
    "objectID": "posts/20191227-learning-dax/index.html#dax-studio",
    "href": "posts/20191227-learning-dax/index.html#dax-studio",
    "title": "Data Analysis eXpressions query language",
    "section": "DAX Studio",
    "text": "DAX Studio\nDAX Studio is a tool to write, execute, and analyze DAX queries in Power BI, Power Pivot for Excel, and Analysis Services Tabular. It can connect to any of sources and access their data model, such as an open Power BI file.\nIt includes an Object Browser, query editing and execution, formula and measure editing, syntax highlighting and formatting, integrated tracing and query execution breakdowns.\nThis tool supercharges you report development toolbelt and allows to access fairly advanced features such as:\n\nQuery tracing and server timings: provides insights on execution time, and differentiates between time spent on formula or query engine to facilitate debugging.\nFilter dumps: you can create a measure automatically that shows the current filter context for each visual and data point. This filter dump can be set on a tooltip to make debugging infinitely easier.\nMeasure definitions: similarly, you can automatically document all of your calculated measures with a single click inside of DAX Studio.\nLanguage formatting: you can also format your DAX queries using the syntax tree developed by SQLBI and used in DAX Formatter.\nDynamic Management Views (DMVs): the tool also grants access to several pre-defined DMVs that allow for finer control of the data model embedded in the file. For example, you can automate the documentation and definition of your model using a series of queries against the DMVs."
  },
  {
    "objectID": "posts/20191227-learning-dax/index.html#closing-thoughts",
    "href": "posts/20191227-learning-dax/index.html#closing-thoughts",
    "title": "Data Analysis eXpressions query language",
    "section": "Closing thoughts",
    "text": "Closing thoughts\nThis is but a small subset of all the capabilities of DAX and merely scratches the surface of the language. Some resources like sqlbi.com or dax.guide are great steps to take next.\nIf you are stuck with a certain problem, try looking or posting a question on the DAX tag in the Power BI community forums. They are a friendly and knowledgeable bunch that will most certainly help you as long as you follow the site’s rules and etiquette.\nHopefully the techniques and tools mentioned in this post as well as the simplicity of the language and it’s Excel-like functionality encourage you to try the language or dive deeper into its guts. In any case, I’m more than happy to answer any questions you might have."
  },
  {
    "objectID": "posts/20250723-switching-to-quarto/index.html",
    "href": "posts/20250723-switching-to-quarto/index.html",
    "title": "Why I Migrated My Blog from Pelican to Quarto",
    "section": "",
    "text": "For years, my blog ran on Pelican, a static site generator written in Python. It was powerful, flexible, and a constant source of frustration. I found myself spending more time wrestling with configurations, fixing broken themes, and making sacrifices to the dependency gods than actually writing.\nI wanted a change, and I wanted a tech stack that would let me focus on content creation, not on web design and endless config files. I wanted something self-hosted (f*ck off, Medium), data science-friendly, and straightforward to maintain.\nSo while I was browsing some random statistics blog, I was very lucky to rediscover Quarto from my R days!\n\n\nI first stumbled upon Quarto while looking for better ways to create slide decks from my Jupyter notebooks. It’s an open-source scientific and technical publishing system developed by the team behind RStudio (now Posit). It’s built on Pandoc and allows you to create dynamic content with Python, R and Julia.\nIt’s fantastic for presentations and reports, but it’s also designed to build entire websites.\nSo, why did I finally ghost Pelican?\n\nConfig and Dependency Hell: Pelican is flexible, but that flexibility comes at the cost of complex configuration files. Setting up themes, plugins, and even basic metadata felt like a chore. Pelican seemed to be in a constant feud with my other dependencies.\nConvoluted Workflow: My writing process involved a series of clunky steps to convert my notebooks, manage static assets, and preview changes. It was anything but seamless.\nUnmaintained and Broken Themes: The theme I was using became unmaintained. My attempts to customize it led to a cascade of broken CSS and Jinja2 template errors. Web dev is definitely not my strong suit.\n\nQuarto gives me a solution to all these issues with its “convention over configuration” philosophy and makes it incredibly easy to get started and maintain.\n\n\n\nOne of the bests parts of moving to Quarto was the setup. Creating a fully functional blog is literally a one-line command:\nquarto create-project my-awesome-blog --type website:blog\nThis single command scaffolds a complete blog structure for you, with sample posts, an index page, and an about page.\n\n\nYou can write your posts directly in the formats you already know and use:\n\nJupyter Notebooks (.ipynb): This is a game-changer for data scientists. You can write your analysis, generate plots, and write your narrative all in one place. To make a notebook a blog post, you just need to add a raw cell at the very top with the YAML frontmatter.\nQuarto Markdown (.qmd): This is a superset of Pandoc Markdown. It’s clean, simple, and allows you to embed executable code chunks from Python, R, and more, right within your markdown file.\n\nAll your posts go into the posts/ directory. Here’s an example of the YAML frontmatter you’d put in a raw cell at the top of a Jupyter Notebook or at the top of a .qmd file:\n---\ntitle: \"My First Quarto Post\"\nauthor: \"Your Name\"\ndate: \"2025-07-21\"\ncategories: [Python, Data Science, Quarto]\nimage: \"preview.png\"\n---\nI like to create a new Git branch for every new post, which keeps your main branch clean and makes it easier to manage drafts.\ngit checkout -b new-post-on-quarto\ngit commit -am \"feat: add new post on quarto\"\ngit push origin new-post-on-quarto\n# Create a pull request to merge into main when ready\n\n\n\nThe Bootstrap out-of-the-box styling makes customization intuitive for anyone with basic web knowledge, or even less than basic like me. You can easily tweak colors, fonts, and layouts using a simple .scss file. In your project’s root, just create a file named theme.scss and reference it in your _quarto.yml file.\n\n\n\n\n\n\nTip\n\n\n\nYou can use Bootstrap’s components like cards or alerts to make your documents more engaging and professional.\n\n\n# _quarto.yml\nproject:\n  type: website\n\nwebsite:\n  title: \"My Awesome Blog\"\n  theme:\n    light: cosmo\n    dark: darkly\n    # Or for custom themes:\n    # light: theme.scss\n\n\n\nQuarto produces a static website that you can host anywhere, with one of the most popular (and free) options being GitHub Pages, although there are probably others like Netlify.\n\n\n\nSetting up features that were cumbersome in Pelican is trivial in Quarto.\n\nRSS Feed: Just add a few lines to _quarto.yml. Quarto handles the rest.\nComments: You can easily integrate services like Giscus (which uses GitHub Discussions), Utterances, or Hypothesis. For Giscus, you add a comments section to your _quarto.yml.\nAnalytics: Add your Google Analytics tracking ID or, for a more privacy-focused option, use a service like Umami. It’s just another simple addition to the main config file.\nOther Cool Stuff: One of my favorite features is “listings.” You can group posts by category or even create a dedicated series. By adding a series tag to your post’s frontmatter, Quarto can automatically generate a listing page that groups these posts together, showing the reader what else is in the series.\n\n\n\n\n\nMigrating my blog to Quarto has felt liberating. I actually put it off for a long time because I dreaded the process, but once I got to it a year later, I was done in a couple of hours.\nThe entire system is designed to be intuitive and stay out of your way. Writing posts directly in Jupyter notebooks has streamlined my workflow immensely, while style supporting enhanced Markdown for simpler articles. I now spend my time thinking about content, code, and analysis instead of why my theme’s CSS is broken again.\nIf you’re a technical professional who wants to run a blog without much hassle, I can’t recommend Quarto enough. It’s the simple, powerful, and data-science-aware tool I wish I had from the start."
  },
  {
    "objectID": "posts/20250723-switching-to-quarto/index.html#what-is-quarto-and-why-leave-pelican",
    "href": "posts/20250723-switching-to-quarto/index.html#what-is-quarto-and-why-leave-pelican",
    "title": "Why I Migrated My Blog from Pelican to Quarto",
    "section": "",
    "text": "I first stumbled upon Quarto while looking for better ways to create slide decks from my Jupyter notebooks. It’s an open-source scientific and technical publishing system developed by the team behind RStudio (now Posit). It’s built on Pandoc and allows you to create dynamic content with Python, R and Julia.\nIt’s fantastic for presentations and reports, but it’s also designed to build entire websites.\nSo, why did I finally ghost Pelican?\n\nConfig and Dependency Hell: Pelican is flexible, but that flexibility comes at the cost of complex configuration files. Setting up themes, plugins, and even basic metadata felt like a chore. Pelican seemed to be in a constant feud with my other dependencies.\nConvoluted Workflow: My writing process involved a series of clunky steps to convert my notebooks, manage static assets, and preview changes. It was anything but seamless.\nUnmaintained and Broken Themes: The theme I was using became unmaintained. My attempts to customize it led to a cascade of broken CSS and Jinja2 template errors. Web dev is definitely not my strong suit.\n\nQuarto gives me a solution to all these issues with its “convention over configuration” philosophy and makes it incredibly easy to get started and maintain."
  },
  {
    "objectID": "posts/20250723-switching-to-quarto/index.html#getting-started",
    "href": "posts/20250723-switching-to-quarto/index.html#getting-started",
    "title": "Why I Migrated My Blog from Pelican to Quarto",
    "section": "",
    "text": "One of the bests parts of moving to Quarto was the setup. Creating a fully functional blog is literally a one-line command:\nquarto create-project my-awesome-blog --type website:blog\nThis single command scaffolds a complete blog structure for you, with sample posts, an index page, and an about page.\n\n\nYou can write your posts directly in the formats you already know and use:\n\nJupyter Notebooks (.ipynb): This is a game-changer for data scientists. You can write your analysis, generate plots, and write your narrative all in one place. To make a notebook a blog post, you just need to add a raw cell at the very top with the YAML frontmatter.\nQuarto Markdown (.qmd): This is a superset of Pandoc Markdown. It’s clean, simple, and allows you to embed executable code chunks from Python, R, and more, right within your markdown file.\n\nAll your posts go into the posts/ directory. Here’s an example of the YAML frontmatter you’d put in a raw cell at the top of a Jupyter Notebook or at the top of a .qmd file:\n---\ntitle: \"My First Quarto Post\"\nauthor: \"Your Name\"\ndate: \"2025-07-21\"\ncategories: [Python, Data Science, Quarto]\nimage: \"preview.png\"\n---\nI like to create a new Git branch for every new post, which keeps your main branch clean and makes it easier to manage drafts.\ngit checkout -b new-post-on-quarto\ngit commit -am \"feat: add new post on quarto\"\ngit push origin new-post-on-quarto\n# Create a pull request to merge into main when ready\n\n\n\nThe Bootstrap out-of-the-box styling makes customization intuitive for anyone with basic web knowledge, or even less than basic like me. You can easily tweak colors, fonts, and layouts using a simple .scss file. In your project’s root, just create a file named theme.scss and reference it in your _quarto.yml file.\n\n\n\n\n\n\nTip\n\n\n\nYou can use Bootstrap’s components like cards or alerts to make your documents more engaging and professional.\n\n\n# _quarto.yml\nproject:\n  type: website\n\nwebsite:\n  title: \"My Awesome Blog\"\n  theme:\n    light: cosmo\n    dark: darkly\n    # Or for custom themes:\n    # light: theme.scss\n\n\n\nQuarto produces a static website that you can host anywhere, with one of the most popular (and free) options being GitHub Pages, although there are probably others like Netlify.\n\n\n\nSetting up features that were cumbersome in Pelican is trivial in Quarto.\n\nRSS Feed: Just add a few lines to _quarto.yml. Quarto handles the rest.\nComments: You can easily integrate services like Giscus (which uses GitHub Discussions), Utterances, or Hypothesis. For Giscus, you add a comments section to your _quarto.yml.\nAnalytics: Add your Google Analytics tracking ID or, for a more privacy-focused option, use a service like Umami. It’s just another simple addition to the main config file.\nOther Cool Stuff: One of my favorite features is “listings.” You can group posts by category or even create a dedicated series. By adding a series tag to your post’s frontmatter, Quarto can automatically generate a listing page that groups these posts together, showing the reader what else is in the series."
  },
  {
    "objectID": "posts/20250723-switching-to-quarto/index.html#takeaway-focus-on-what-matters",
    "href": "posts/20250723-switching-to-quarto/index.html#takeaway-focus-on-what-matters",
    "title": "Why I Migrated My Blog from Pelican to Quarto",
    "section": "",
    "text": "Migrating my blog to Quarto has felt liberating. I actually put it off for a long time because I dreaded the process, but once I got to it a year later, I was done in a couple of hours.\nThe entire system is designed to be intuitive and stay out of your way. Writing posts directly in Jupyter notebooks has streamlined my workflow immensely, while style supporting enhanced Markdown for simpler articles. I now spend my time thinking about content, code, and analysis instead of why my theme’s CSS is broken again.\nIf you’re a technical professional who wants to run a blog without much hassle, I can’t recommend Quarto enough. It’s the simple, powerful, and data-science-aware tool I wish I had from the start."
  }
]